{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name() if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Attempting to run cuBLAS, but there was no current CUDA context!\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "# torch.manual_seed(333667)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDE Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Identical Kuramoto Equation\n",
      "\n",
      "Domain Limits (xmin, xmax): 0 ,  6.283185307179586\n",
      "Number of Spatial Points (Nx): 512\n",
      "Final Time (Tf): 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Domain Setup \n",
    "xmin, xmax = 0, 2*np.pi     # Spatial domain\n",
    "Nx = 512                    # Spatial resolution\n",
    "\n",
    "Nt = 204                    # Temporal resolution\n",
    "Tf = 1                      # Temporal final time\n",
    "\n",
    "K = 1.0                     # Coupling constant \n",
    "\n",
    "print(\"\\nIdentical Kuramoto Equation\\n\")\n",
    "print(\"Domain Limits (xmin, xmax):\", xmin, \", \", xmax)\n",
    "print(\"Number of Spatial Points (Nx):\", Nx)\n",
    "print(\"Final Time (Tf):\", Tf, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL = None\n",
    "def initialise(theta):\n",
    "    u0 = torch.zeros_like(theta)\n",
    "\n",
    "    if INITIAL=='polynomial':\n",
    "        mask = (theta >= (torch.pi/2)) & (theta <= (3*torch.pi/2))\n",
    "        u0[mask] = 6 / (torch.pi**3) * ((3*torch.pi/2 - theta[mask]) * (theta[mask] - torch.pi/2))\n",
    "\n",
    "        print(\"Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\\n\")\n",
    "    \n",
    "    elif INITIAL=='singular':\n",
    "        def dirac_delta(x, a, epsilon=0.01):\n",
    "            return torch.where(torch.abs(x-a) < epsilon, 1/(2*epsilon), 0)\n",
    "\n",
    "        def heaviside(x):\n",
    "            return torch.where(x>=0, 1, 0)\n",
    "        \n",
    "        u0[:] = ( 1/4 * (dirac_delta(theta, 3*torch.pi/4) + dirac_delta(theta, 5*torch.pi/4)) +\n",
    "                1/2 * heaviside(theta - torch.pi/2) * (1 - heaviside(theta - 3*torch.pi/2)) )\n",
    "\n",
    "        print(\"Dirac Singularities: u_0(x) = 1/4 * (delta_3pi/4 + delta_5pi/4) + 1/2 * X_[pi/2, 3pi/2] \")\n",
    "\n",
    "    elif INITIAL=='piecewise':\n",
    "        mask = (theta >= (torch.pi/2)) & (theta <= (3*torch.pi/2))\n",
    "        u0[mask]  = 2.0 / (3.0*torch.pi)\n",
    "        u0[~mask] = 1.0 / (3.0*torch.pi)\n",
    "\n",
    "        print(\"Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\\n\")\n",
    "\n",
    "    return u0.to(device)\n",
    "\n",
    "\n",
    "def nonlocal_flux(model, theta, t, Nq=100):\n",
    "    \"\"\"\n",
    "    For each collocation point (theta, t) approximate\n",
    "       L[p](x,t) = -K * integral|0^(2pi)(sin(x - x') p(x', t) dx')\n",
    "    using a Riemann sum.\n",
    "    \"\"\"\n",
    "    # Create quadrature points for x' in [xmin, xmax]\n",
    "    theta_prime = torch.linspace(xmin, xmax, Nq, device=device).view(1, Nq)  # shape (1, Nq)\n",
    "    delta_theta = xmax / Nq \n",
    "\n",
    "    # Expand to (N, Nq)\n",
    "    theta_exp = theta.expand(-1, Nq)\n",
    "    t_exp = t.expand(-1, Nq)\n",
    "    \n",
    "    inp_integ = torch.cat([theta_prime.repeat(theta.shape[0], 1).view(-1, 1),\n",
    "                           t_exp.reshape(-1, 1)], dim=1)  # shape (N*Nq, 2)\n",
    "\n",
    "    # Evaluate p\n",
    "    rho_integ = model(inp_integ)  # shape (N*Nq, 1)\n",
    "    rho_integ = rho_integ.view(theta.shape[0], Nq)  # shape (N, Nq)\n",
    "\n",
    "    # Compute integrand: sin(x - x') * p(x',t)\n",
    "    integrand = torch.sin(theta_exp - theta_prime.expand(theta_exp.shape)) * rho_integ  # shape (N, Nq)\n",
    "    \n",
    "    # Riemann sum\n",
    "    integral = torch.sum(integrand, dim=1, keepdim=True) * delta_theta  # shape (N, 1)\n",
    "    \n",
    "    L = -K * integral\n",
    "    return L\n",
    "\n",
    "\n",
    "def pde_residual(model, theta, t):\n",
    "    \"\"\"\n",
    "    compute the PDE residual:\n",
    "         f = dp(x,t) + dx(L[p](x,t)*p(x,t)).\n",
    "    \"\"\"\n",
    "    # For collocation points (theta, t) (each (N,1)) with theta.requires_grad=True and t.requires_grad=True,    \n",
    "    theta.requires_grad_(True)\n",
    "    t.requires_grad_(True)\n",
    "    \n",
    "    x = torch.cat([theta, t], dim=1)  # (N,2)\n",
    "    rho = model(x)  # (N,1)\n",
    "\n",
    "    rho_t = torch.autograd.grad(rho, t, grad_outputs=torch.ones_like(rho),\n",
    "                                retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    L_rho = nonlocal_flux(model, theta, t)\n",
    "    u = L_rho * rho\n",
    "\n",
    "    u_theta = torch.autograd.grad(u, theta, grad_outputs=torch.ones_like(u),\n",
    "                                  retain_graph=True, create_graph=True)[0]\n",
    "    \n",
    "    # PDE residual: p_t + dx(u)\n",
    "    f = rho_t + u_theta\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture\n",
    "### Simple FeedForward Neural Network\n",
    "\n",
    "-> 1 input layer\n",
    "\n",
    "-> 4 fully connected hidden layers; 64 neurons each\n",
    "\n",
    "-> 1 output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_collocation = 1024  # collocation points for enforcing the PDE\n",
    "\n",
    "# Collocation points in the interior: (x in [0, xmax], t in (0, Tf])\n",
    "theta_colloc = torch.rand(N_collocation, 1, device=device) * xmax\n",
    "t_colloc = torch.rand(N_collocation, 1, device=device) * Tf\n",
    "\n",
    "# for initalise\n",
    "theta_init = torch.linspace(xmin, xmax, Nx, device=device).view(-1, 1)\n",
    "t_init = torch.zeros_like(theta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self, activation, hidden_layers=4, neurons=64):\n",
    "        super(PINN, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(2, neurons)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(neurons, neurons) for _ in range(hidden_layers)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(neurons, 1)\n",
    "        \n",
    "        self.activation = activation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N,2) with x[:, 0]=x and x[:, 1]=t\n",
    "        out = self.input_layer(x)\n",
    "        out = self.activation(out)\n",
    "        for layer in self.hidden_layers:\n",
    "            out = layer(out)\n",
    "            out = self.activation(out)\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def trainer(u_0, model, optimizer, num_epochs=10240):\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # PDE residual loss\n",
    "        f = pde_residual(model, theta_colloc, t_colloc)\n",
    "        loss_pde = torch.mean(f**2)\n",
    "        \n",
    "        # Initial condition loss\n",
    "        x_init = torch.cat([theta_init, t_init], dim=1)\n",
    "        u_0_pred = model(x_init)\n",
    "        loss_ic = torch.mean((u_0_pred - u_0)**2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_pde + loss_ic\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        if epoch % 512 == 0:\n",
    "            print(f\"Epoch {epoch}, Total Loss: {loss.item():.6f}, PDE Loss: {loss_pde.item():.6f}, IC Loss: {loss_ic.item():.6f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test = torch.linspace(xmin, xmax, Nx, device=device)\n",
    "t_test = torch.linspace(0, Tf, Nt, device=device)\n",
    "\n",
    "Theta, T = torch.meshgrid(theta_test, t_test, indexing='ij')  # Theta: (Nx, Nt), T: (Nx, Nt)\n",
    "\n",
    "# Input (flattened)\n",
    "x_test = torch.cat([Theta.reshape(-1, 1), T.reshape(-1, 1)], dim=1)\n",
    "\n",
    "\n",
    "# Plot prediciton\n",
    "t_test_np = t_test.cpu().numpy()\n",
    "x_line = theta_test.cpu().numpy()\n",
    "Theta_np = Theta.cpu().numpy()\n",
    "T_np = T.cpu().numpy()\n",
    "def plot(u_n):\n",
    "    # Contour \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cp = plt.contourf(Theta_np, T_np, u_n, levels=50, cmap='viridis')\n",
    "    plt.colorbar(cp, label='p(x,t)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('t')\n",
    "    plt.title(f'Evolution of u(x, t) from t=0 to t={Tf}')\n",
    "    plt.show()\n",
    "\n",
    "    # Line plots\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    time_snapshots = [0.0, Tf/4, Tf/2, 3*Tf/4, Tf]\n",
    "    for ts in time_snapshots:\n",
    "        idx = np.argmin(np.abs(t_test_np - ts))\n",
    "        plt.plot(x_line, u_n[:, idx], label=f\"t={t_test_np[idx]:.2f}\")\n",
    "\n",
    "    plt.xlabel(r'$\\theta$')\n",
    "    plt.ylabel(r'$\\rho(\\theta)$')\n",
    "    plt.title(\"Solution Evolution with time\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tanh, 4, 64, 4096, 1024, Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PINN(activation=torch.tanh, hidden_layers=4, neurons=64).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# # print(summary(model), '\\n')\n",
    "\n",
    "# INITIAL = 'polynomial'\n",
    "# u0 = initialise(theta_init)\n",
    "\n",
    "# # trainer(u0, model, optimizer, num_epochs=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "    # u_n = model(x_test).reshape(Nx, Nt).cpu().numpy()\n",
    "    # print(u_n.shape)\n",
    "\n",
    "# plot(u_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps_plot(u_n, exact_file='u_ex.txt', xmin=0, xmax=2*np.pi, Tf=1):\n",
    "    N = u_n.shape[0]\n",
    "    x = np.linspace(xmin, xmax, N)\n",
    "    \n",
    "    time_steps = u_n.shape[1]    \n",
    "    for t in range(time_steps):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.clf()\n",
    "        plt.xlabel(r'$\\theta$')\n",
    "        plt.ylabel(r'$rho(\\theta)$')\n",
    "        plt.title('PINN Simulation; IDKuramoto, Polynomial Initial Data')\n",
    "        \n",
    "        if os.path.exists(exact_file) and os.path.getsize(exact_file) > 0:\n",
    "            ex_data = np.loadtxt(exact_file).transpose()\n",
    "            plt.plot(x, ex_data[:, t], linestyle=\":\", marker=\"o\", markersize=1,\n",
    "                     markerfacecolor='none', label='Exact Solution')\n",
    "        \n",
    "        plt.plot(x, u_n[:, t], linestyle=\":\", marker=\"o\", markersize=1,\n",
    "                 markerfacecolor='none', label='PINN Solution')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.savefig(rf\"steps/{t}.png\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# steps_plot(u_n, xmin=xmin, xmax=xmax, Tf=Tf)\n",
    "\n",
    "# !python steps/gif.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "def _trainer(u_0, model, optimizer, theta_colloc, t_colloc, num_epochs=10240):\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute PDE residual loss\n",
    "        f = pde_residual(model, theta_colloc, t_colloc)\n",
    "        loss_pde = torch.mean(f**2)\n",
    "        \n",
    "        # Compute initial condition loss\n",
    "        x_init = torch.cat([theta_init, t_init], dim=1)\n",
    "        u0_pred = model(x_init)\n",
    "        loss_ic = torch.mean((u0_pred - u_0)**2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_pde + loss_ic\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        if epoch % 512 == 0:\n",
    "            print(f\"Epoch {epoch}, Total Loss: {loss.item():.6f}, PDE Loss: {loss_pde.item():.6f}, IC Loss: {loss_ic.item():.6f}\")\n",
    "    \n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.081796, PDE Loss: 0.000022, IC Loss: 0.081774\n",
      "Epoch 512, Total Loss: 0.003117, PDE Loss: 0.001662, IC Loss: 0.001454\n",
      "Epoch 1024, Total Loss: 0.001640, PDE Loss: 0.001018, IC Loss: 0.000622\n",
      "Epoch 1536, Total Loss: 0.000319, PDE Loss: 0.000191, IC Loss: 0.000129\n",
      "Training completed in 34.68 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.090133, PDE Loss: 0.000400, IC Loss: 0.089733\n",
      "Epoch 512, Total Loss: 0.014805, PDE Loss: 0.006298, IC Loss: 0.008507\n",
      "Epoch 1024, Total Loss: 0.003397, PDE Loss: 0.001817, IC Loss: 0.001581\n",
      "Epoch 1536, Total Loss: 0.001090, PDE Loss: 0.000642, IC Loss: 0.000447\n",
      "Training completed in 62.24 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.038774, PDE Loss: 0.000091, IC Loss: 0.038682\n",
      "Epoch 512, Total Loss: 0.010514, PDE Loss: 0.003790, IC Loss: 0.006724\n",
      "Epoch 1024, Total Loss: 0.006340, PDE Loss: 0.002903, IC Loss: 0.003437\n",
      "Epoch 1536, Total Loss: 0.001321, PDE Loss: 0.000664, IC Loss: 0.000657\n",
      "Epoch 2048, Total Loss: 0.000488, PDE Loss: 0.000302, IC Loss: 0.000186\n",
      "Epoch 2560, Total Loss: 0.000186, PDE Loss: 0.000114, IC Loss: 0.000073\n",
      "Epoch 3072, Total Loss: 0.000122, PDE Loss: 0.000066, IC Loss: 0.000056\n",
      "Epoch 3584, Total Loss: 0.000134, PDE Loss: 0.000078, IC Loss: 0.000056\n",
      "Training completed in 67.24 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.034804, PDE Loss: 0.000207, IC Loss: 0.034597\n",
      "Epoch 512, Total Loss: 0.007410, PDE Loss: 0.003008, IC Loss: 0.004401\n",
      "Epoch 1024, Total Loss: 0.003235, PDE Loss: 0.001878, IC Loss: 0.001357\n",
      "Epoch 1536, Total Loss: 0.001374, PDE Loss: 0.000766, IC Loss: 0.000608\n",
      "Epoch 2048, Total Loss: 0.000476, PDE Loss: 0.000286, IC Loss: 0.000191\n",
      "Epoch 2560, Total Loss: 0.000220, PDE Loss: 0.000124, IC Loss: 0.000096\n",
      "Epoch 3072, Total Loss: 0.000140, PDE Loss: 0.000076, IC Loss: 0.000064\n",
      "Epoch 3584, Total Loss: 0.000099, PDE Loss: 0.000048, IC Loss: 0.000052\n",
      "Training completed in 126.28 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.082016, PDE Loss: 0.000181, IC Loss: 0.081834\n",
      "Epoch 512, Total Loss: 0.005889, PDE Loss: 0.003413, IC Loss: 0.002476\n",
      "Epoch 1024, Total Loss: 0.001031, PDE Loss: 0.000619, IC Loss: 0.000412\n",
      "Epoch 1536, Total Loss: 0.000333, PDE Loss: 0.000187, IC Loss: 0.000146\n",
      "Epoch 2048, Total Loss: 0.000205, PDE Loss: 0.000114, IC Loss: 0.000091\n",
      "Epoch 2560, Total Loss: 0.000149, PDE Loss: 0.000084, IC Loss: 0.000065\n",
      "Epoch 3072, Total Loss: 0.000123, PDE Loss: 0.000071, IC Loss: 0.000052\n",
      "Epoch 3584, Total Loss: 0.000329, PDE Loss: 0.000228, IC Loss: 0.000101\n",
      "Epoch 4096, Total Loss: 0.000100, PDE Loss: 0.000060, IC Loss: 0.000040\n",
      "Epoch 4608, Total Loss: 0.000056, PDE Loss: 0.000027, IC Loss: 0.000029\n",
      "Training completed in 86.39 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.172910, PDE Loss: 0.000067, IC Loss: 0.172844\n",
      "Epoch 512, Total Loss: 0.005160, PDE Loss: 0.001875, IC Loss: 0.003284\n",
      "Epoch 1024, Total Loss: 0.000749, PDE Loss: 0.000428, IC Loss: 0.000321\n",
      "Epoch 1536, Total Loss: 0.000229, PDE Loss: 0.000120, IC Loss: 0.000108\n",
      "Epoch 2048, Total Loss: 0.000159, PDE Loss: 0.000084, IC Loss: 0.000075\n",
      "Epoch 2560, Total Loss: 0.000128, PDE Loss: 0.000065, IC Loss: 0.000063\n",
      "Epoch 3072, Total Loss: 0.000103, PDE Loss: 0.000048, IC Loss: 0.000055\n",
      "Epoch 3584, Total Loss: 0.000086, PDE Loss: 0.000038, IC Loss: 0.000049\n",
      "Epoch 4096, Total Loss: 0.000104, PDE Loss: 0.000041, IC Loss: 0.000063\n",
      "Epoch 4608, Total Loss: 0.000057, PDE Loss: 0.000022, IC Loss: 0.000035\n",
      "Training completed in 160.38 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.120845, PDE Loss: 0.000047, IC Loss: 0.120798\n",
      "Epoch 512, Total Loss: 0.007403, PDE Loss: 0.001876, IC Loss: 0.005527\n",
      "Epoch 1024, Total Loss: 0.000663, PDE Loss: 0.000391, IC Loss: 0.000272\n",
      "Epoch 1536, Total Loss: 0.000247, PDE Loss: 0.000130, IC Loss: 0.000116\n",
      "Epoch 2048, Total Loss: 0.000142, PDE Loss: 0.000070, IC Loss: 0.000072\n",
      "Epoch 2560, Total Loss: 0.000104, PDE Loss: 0.000050, IC Loss: 0.000054\n",
      "Epoch 3072, Total Loss: 0.000084, PDE Loss: 0.000040, IC Loss: 0.000045\n",
      "Epoch 3584, Total Loss: 0.000693, PDE Loss: 0.000519, IC Loss: 0.000174\n",
      "Epoch 4096, Total Loss: 0.001349, PDE Loss: 0.001092, IC Loss: 0.000256\n",
      "Epoch 4608, Total Loss: 0.000084, PDE Loss: 0.000049, IC Loss: 0.000036\n",
      "Epoch 5120, Total Loss: 0.000054, PDE Loss: 0.000023, IC Loss: 0.000031\n",
      "Epoch 5632, Total Loss: 0.000065, PDE Loss: 0.000036, IC Loss: 0.000030\n",
      "Epoch 6144, Total Loss: 0.000058, PDE Loss: 0.000031, IC Loss: 0.000026\n",
      "Epoch 6656, Total Loss: 0.000229, PDE Loss: 0.000110, IC Loss: 0.000119\n",
      "Epoch 7168, Total Loss: 0.000059, PDE Loss: 0.000035, IC Loss: 0.000025\n",
      "Epoch 7680, Total Loss: 0.000027, PDE Loss: 0.000011, IC Loss: 0.000016\n",
      "Epoch 8192, Total Loss: 0.000035, PDE Loss: 0.000018, IC Loss: 0.000016\n",
      "Epoch 8704, Total Loss: 0.000060, PDE Loss: 0.000041, IC Loss: 0.000019\n",
      "Epoch 9216, Total Loss: 0.000050, PDE Loss: 0.000034, IC Loss: 0.000016\n",
      "Epoch 9728, Total Loss: 0.000016, PDE Loss: 0.000007, IC Loss: 0.000009\n",
      "Training completed in 171.66 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=64, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.086557, PDE Loss: 0.000097, IC Loss: 0.086460\n",
      "Epoch 512, Total Loss: 0.007356, PDE Loss: 0.003039, IC Loss: 0.004317\n",
      "Epoch 1024, Total Loss: 0.001672, PDE Loss: 0.000631, IC Loss: 0.001040\n",
      "Epoch 1536, Total Loss: 0.000385, PDE Loss: 0.000217, IC Loss: 0.000168\n",
      "Epoch 2048, Total Loss: 0.000209, PDE Loss: 0.000102, IC Loss: 0.000107\n",
      "Epoch 2560, Total Loss: 0.000146, PDE Loss: 0.000068, IC Loss: 0.000078\n",
      "Epoch 3072, Total Loss: 0.000135, PDE Loss: 0.000062, IC Loss: 0.000073\n",
      "Epoch 3584, Total Loss: 0.000094, PDE Loss: 0.000038, IC Loss: 0.000056\n",
      "Epoch 4096, Total Loss: 0.000339, PDE Loss: 0.000261, IC Loss: 0.000078\n",
      "Epoch 4608, Total Loss: 0.000323, PDE Loss: 0.000233, IC Loss: 0.000090\n",
      "Epoch 5120, Total Loss: 0.000150, PDE Loss: 0.000078, IC Loss: 0.000072\n",
      "Epoch 5632, Total Loss: 0.000146, PDE Loss: 0.000100, IC Loss: 0.000046\n",
      "Epoch 6144, Total Loss: 0.000081, PDE Loss: 0.000046, IC Loss: 0.000035\n",
      "Epoch 6656, Total Loss: 0.000097, PDE Loss: 0.000063, IC Loss: 0.000034\n",
      "Epoch 7168, Total Loss: 0.000321, PDE Loss: 0.000235, IC Loss: 0.000086\n",
      "Epoch 7680, Total Loss: 0.000039, PDE Loss: 0.000016, IC Loss: 0.000022\n",
      "Epoch 8192, Total Loss: 0.000031, PDE Loss: 0.000012, IC Loss: 0.000020\n",
      "Epoch 8704, Total Loss: 0.000030, PDE Loss: 0.000011, IC Loss: 0.000019\n",
      "Epoch 9216, Total Loss: 0.000053, PDE Loss: 0.000019, IC Loss: 0.000034\n",
      "Epoch 9728, Total Loss: 0.000025, PDE Loss: 0.000009, IC Loss: 0.000016\n",
      "Training completed in 320.13 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.073939, PDE Loss: 0.000311, IC Loss: 0.073628\n",
      "Epoch 512, Total Loss: 0.002119, PDE Loss: 0.001209, IC Loss: 0.000911\n",
      "Epoch 1024, Total Loss: 0.000295, PDE Loss: 0.000181, IC Loss: 0.000114\n",
      "Epoch 1536, Total Loss: 0.000163, PDE Loss: 0.000088, IC Loss: 0.000075\n",
      "Training completed in 58.80 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.048629, PDE Loss: 0.000128, IC Loss: 0.048501\n",
      "Epoch 512, Total Loss: 0.001875, PDE Loss: 0.001068, IC Loss: 0.000806\n",
      "Epoch 1024, Total Loss: 0.005819, PDE Loss: 0.004047, IC Loss: 0.001772\n",
      "Epoch 1536, Total Loss: 0.000104, PDE Loss: 0.000051, IC Loss: 0.000053\n",
      "Training completed in 113.79 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.045591, PDE Loss: 0.000404, IC Loss: 0.045187\n",
      "Epoch 512, Total Loss: 0.004072, PDE Loss: 0.002091, IC Loss: 0.001981\n",
      "Epoch 1024, Total Loss: 0.000326, PDE Loss: 0.000207, IC Loss: 0.000118\n",
      "Epoch 1536, Total Loss: 0.000109, PDE Loss: 0.000063, IC Loss: 0.000046\n",
      "Epoch 2048, Total Loss: 0.000068, PDE Loss: 0.000034, IC Loss: 0.000034\n",
      "Epoch 2560, Total Loss: 0.000071, PDE Loss: 0.000040, IC Loss: 0.000031\n",
      "Epoch 3072, Total Loss: 0.000154, PDE Loss: 0.000102, IC Loss: 0.000052\n",
      "Epoch 3584, Total Loss: 0.000129, PDE Loss: 0.000064, IC Loss: 0.000065\n",
      "Training completed in 113.41 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.050732, PDE Loss: 0.000218, IC Loss: 0.050514\n",
      "Epoch 512, Total Loss: 0.002754, PDE Loss: 0.001406, IC Loss: 0.001348\n",
      "Epoch 1024, Total Loss: 0.000472, PDE Loss: 0.000290, IC Loss: 0.000181\n",
      "Epoch 1536, Total Loss: 0.001685, PDE Loss: 0.001144, IC Loss: 0.000541\n",
      "Epoch 2048, Total Loss: 0.000414, PDE Loss: 0.000183, IC Loss: 0.000231\n",
      "Epoch 2560, Total Loss: 0.000220, PDE Loss: 0.000114, IC Loss: 0.000105\n",
      "Epoch 3072, Total Loss: 0.000188, PDE Loss: 0.000083, IC Loss: 0.000105\n",
      "Epoch 3584, Total Loss: 0.000501, PDE Loss: 0.000362, IC Loss: 0.000139\n",
      "Training completed in 215.60 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.062900, PDE Loss: 0.000421, IC Loss: 0.062480\n",
      "Epoch 512, Total Loss: 0.001487, PDE Loss: 0.000888, IC Loss: 0.000599\n",
      "Epoch 1024, Total Loss: 0.000187, PDE Loss: 0.000099, IC Loss: 0.000088\n",
      "Epoch 1536, Total Loss: 0.000447, PDE Loss: 0.000327, IC Loss: 0.000119\n",
      "Epoch 2048, Total Loss: 0.000135, PDE Loss: 0.000065, IC Loss: 0.000070\n",
      "Epoch 2560, Total Loss: 0.000050, PDE Loss: 0.000023, IC Loss: 0.000027\n",
      "Epoch 3072, Total Loss: 0.000077, PDE Loss: 0.000048, IC Loss: 0.000029\n",
      "Epoch 3584, Total Loss: 0.000143, PDE Loss: 0.000077, IC Loss: 0.000066\n",
      "Epoch 4096, Total Loss: 0.000147, PDE Loss: 0.000113, IC Loss: 0.000035\n",
      "Epoch 4608, Total Loss: 0.000034, PDE Loss: 0.000020, IC Loss: 0.000014\n",
      "Training completed in 141.78 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.033454, PDE Loss: 0.000105, IC Loss: 0.033349\n",
      "Epoch 512, Total Loss: 0.004688, PDE Loss: 0.001888, IC Loss: 0.002800\n",
      "Epoch 1024, Total Loss: 0.000664, PDE Loss: 0.000398, IC Loss: 0.000266\n",
      "Epoch 1536, Total Loss: 0.000276, PDE Loss: 0.000144, IC Loss: 0.000131\n",
      "Epoch 2048, Total Loss: 0.000193, PDE Loss: 0.000081, IC Loss: 0.000113\n",
      "Epoch 2560, Total Loss: 0.000114, PDE Loss: 0.000045, IC Loss: 0.000069\n",
      "Epoch 3072, Total Loss: 0.000107, PDE Loss: 0.000041, IC Loss: 0.000066\n",
      "Epoch 3584, Total Loss: 0.000095, PDE Loss: 0.000045, IC Loss: 0.000050\n",
      "Epoch 4096, Total Loss: 0.000083, PDE Loss: 0.000033, IC Loss: 0.000050\n",
      "Epoch 4608, Total Loss: 0.000109, PDE Loss: 0.000065, IC Loss: 0.000044\n",
      "Training completed in 274.80 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.054303, PDE Loss: 0.000049, IC Loss: 0.054254\n",
      "Epoch 512, Total Loss: 0.002671, PDE Loss: 0.001471, IC Loss: 0.001200\n",
      "Epoch 1024, Total Loss: 0.000963, PDE Loss: 0.000518, IC Loss: 0.000445\n",
      "Epoch 1536, Total Loss: 0.000120, PDE Loss: 0.000060, IC Loss: 0.000060\n",
      "Epoch 2048, Total Loss: 0.000213, PDE Loss: 0.000150, IC Loss: 0.000063\n",
      "Epoch 2560, Total Loss: 0.000179, PDE Loss: 0.000122, IC Loss: 0.000057\n",
      "Epoch 3072, Total Loss: 0.000048, PDE Loss: 0.000026, IC Loss: 0.000022\n",
      "Epoch 3584, Total Loss: 0.000037, PDE Loss: 0.000018, IC Loss: 0.000020\n",
      "Epoch 4096, Total Loss: 0.000031, PDE Loss: 0.000015, IC Loss: 0.000016\n",
      "Epoch 4608, Total Loss: 0.000043, PDE Loss: 0.000028, IC Loss: 0.000014\n",
      "Epoch 5120, Total Loss: 0.000049, PDE Loss: 0.000033, IC Loss: 0.000016\n",
      "Epoch 5632, Total Loss: 0.000089, PDE Loss: 0.000065, IC Loss: 0.000024\n",
      "Epoch 6144, Total Loss: 0.000034, PDE Loss: 0.000023, IC Loss: 0.000012\n",
      "Epoch 6656, Total Loss: 0.000137, PDE Loss: 0.000078, IC Loss: 0.000059\n",
      "Epoch 7168, Total Loss: 0.000025, PDE Loss: 0.000016, IC Loss: 0.000009\n",
      "Epoch 7680, Total Loss: 0.000078, PDE Loss: 0.000056, IC Loss: 0.000022\n",
      "Epoch 8192, Total Loss: 0.000012, PDE Loss: 0.000007, IC Loss: 0.000006\n",
      "Epoch 8704, Total Loss: 0.010783, PDE Loss: 0.003277, IC Loss: 0.007506\n",
      "Epoch 9216, Total Loss: 0.000012, PDE Loss: 0.000007, IC Loss: 0.000005\n",
      "Epoch 9728, Total Loss: 0.000009, PDE Loss: 0.000004, IC Loss: 0.000004\n",
      "Training completed in 293.05 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=4, neurons=128, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.126743, PDE Loss: 0.000050, IC Loss: 0.126694\n",
      "Epoch 512, Total Loss: 0.005188, PDE Loss: 0.002977, IC Loss: 0.002211\n",
      "Epoch 1024, Total Loss: 0.000638, PDE Loss: 0.000386, IC Loss: 0.000252\n",
      "Epoch 1536, Total Loss: 0.001707, PDE Loss: 0.001353, IC Loss: 0.000354\n",
      "Epoch 2048, Total Loss: 0.000173, PDE Loss: 0.000078, IC Loss: 0.000095\n",
      "Epoch 2560, Total Loss: 0.000219, PDE Loss: 0.000133, IC Loss: 0.000086\n",
      "Epoch 3072, Total Loss: 0.000118, PDE Loss: 0.000052, IC Loss: 0.000067\n",
      "Epoch 3584, Total Loss: 0.000389, PDE Loss: 0.000287, IC Loss: 0.000102\n",
      "Epoch 4096, Total Loss: 0.000438, PDE Loss: 0.000332, IC Loss: 0.000107\n",
      "Epoch 4608, Total Loss: 0.000078, PDE Loss: 0.000034, IC Loss: 0.000044\n",
      "Epoch 5120, Total Loss: 0.000208, PDE Loss: 0.000139, IC Loss: 0.000069\n",
      "Epoch 5632, Total Loss: 0.000113, PDE Loss: 0.000068, IC Loss: 0.000045\n",
      "Epoch 6144, Total Loss: 0.000075, PDE Loss: 0.000040, IC Loss: 0.000034\n",
      "Epoch 6656, Total Loss: 0.000046, PDE Loss: 0.000024, IC Loss: 0.000022\n",
      "Epoch 7168, Total Loss: 0.000080, PDE Loss: 0.000053, IC Loss: 0.000026\n",
      "Epoch 7680, Total Loss: 0.000024, PDE Loss: 0.000012, IC Loss: 0.000013\n",
      "Epoch 8192, Total Loss: 0.000014, PDE Loss: 0.000006, IC Loss: 0.000007\n",
      "Epoch 8704, Total Loss: 0.000029, PDE Loss: 0.000016, IC Loss: 0.000013\n",
      "Epoch 9216, Total Loss: 0.000681, PDE Loss: 0.000406, IC Loss: 0.000275\n",
      "Epoch 9728, Total Loss: 0.000112, PDE Loss: 0.000062, IC Loss: 0.000050\n",
      "Training completed in 560.18 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.077217, PDE Loss: 0.000016, IC Loss: 0.077201\n",
      "Epoch 512, Total Loss: 0.001981, PDE Loss: 0.000950, IC Loss: 0.001032\n",
      "Epoch 1024, Total Loss: 0.000153, PDE Loss: 0.000092, IC Loss: 0.000061\n",
      "Epoch 1536, Total Loss: 0.000107, PDE Loss: 0.000059, IC Loss: 0.000047\n",
      "Training completed in 81.50 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.055179, PDE Loss: 0.000014, IC Loss: 0.055165\n",
      "Epoch 512, Total Loss: 0.009310, PDE Loss: 0.003768, IC Loss: 0.005542\n",
      "Epoch 1024, Total Loss: 0.000269, PDE Loss: 0.000160, IC Loss: 0.000109\n",
      "Epoch 1536, Total Loss: 0.000194, PDE Loss: 0.000130, IC Loss: 0.000063\n",
      "Training completed in 157.26 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.043879, PDE Loss: 0.000031, IC Loss: 0.043849\n",
      "Epoch 512, Total Loss: 0.007608, PDE Loss: 0.001952, IC Loss: 0.005657\n",
      "Epoch 1024, Total Loss: 0.000116, PDE Loss: 0.000055, IC Loss: 0.000061\n",
      "Epoch 1536, Total Loss: 0.000085, PDE Loss: 0.000040, IC Loss: 0.000045\n",
      "Epoch 2048, Total Loss: 0.000141, PDE Loss: 0.000089, IC Loss: 0.000053\n",
      "Epoch 2560, Total Loss: 0.000039, PDE Loss: 0.000017, IC Loss: 0.000023\n",
      "Epoch 3072, Total Loss: 0.000435, PDE Loss: 0.000331, IC Loss: 0.000104\n",
      "Epoch 3584, Total Loss: 0.000399, PDE Loss: 0.000301, IC Loss: 0.000098\n",
      "Training completed in 163.15 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.087347, PDE Loss: 0.000102, IC Loss: 0.087245\n",
      "Epoch 512, Total Loss: 0.002711, PDE Loss: 0.001742, IC Loss: 0.000968\n",
      "Epoch 1024, Total Loss: 0.001672, PDE Loss: 0.001259, IC Loss: 0.000413\n",
      "Epoch 1536, Total Loss: 0.000158, PDE Loss: 0.000078, IC Loss: 0.000079\n",
      "Epoch 2048, Total Loss: 0.002094, PDE Loss: 0.001474, IC Loss: 0.000619\n",
      "Epoch 2560, Total Loss: 0.000098, PDE Loss: 0.000049, IC Loss: 0.000050\n",
      "Epoch 3072, Total Loss: 0.001120, PDE Loss: 0.000718, IC Loss: 0.000403\n",
      "Epoch 3584, Total Loss: 0.000062, PDE Loss: 0.000030, IC Loss: 0.000032\n",
      "Training completed in 314.08 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.087301, PDE Loss: 0.000017, IC Loss: 0.087284\n",
      "Epoch 512, Total Loss: 0.002543, PDE Loss: 0.001165, IC Loss: 0.001378\n",
      "Epoch 1024, Total Loss: 0.001778, PDE Loss: 0.000876, IC Loss: 0.000902\n",
      "Epoch 1536, Total Loss: 0.000109, PDE Loss: 0.000051, IC Loss: 0.000058\n",
      "Epoch 2048, Total Loss: 0.000183, PDE Loss: 0.000120, IC Loss: 0.000064\n",
      "Epoch 2560, Total Loss: 0.000055, PDE Loss: 0.000024, IC Loss: 0.000031\n",
      "Epoch 3072, Total Loss: 0.000087, PDE Loss: 0.000055, IC Loss: 0.000032\n",
      "Epoch 3584, Total Loss: 0.000111, PDE Loss: 0.000078, IC Loss: 0.000033\n",
      "Epoch 4096, Total Loss: 0.000031, PDE Loss: 0.000017, IC Loss: 0.000014\n",
      "Epoch 4608, Total Loss: 0.000032, PDE Loss: 0.000008, IC Loss: 0.000023\n",
      "Training completed in 203.81 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.068902, PDE Loss: 0.000030, IC Loss: 0.068873\n",
      "Epoch 512, Total Loss: 0.001940, PDE Loss: 0.000708, IC Loss: 0.001232\n",
      "Epoch 1024, Total Loss: 0.000369, PDE Loss: 0.000258, IC Loss: 0.000111\n",
      "Epoch 1536, Total Loss: 0.000143, PDE Loss: 0.000068, IC Loss: 0.000075\n",
      "Epoch 2048, Total Loss: 0.000092, PDE Loss: 0.000037, IC Loss: 0.000054\n",
      "Epoch 2560, Total Loss: 0.000404, PDE Loss: 0.000274, IC Loss: 0.000129\n",
      "Epoch 3072, Total Loss: 0.000502, PDE Loss: 0.000398, IC Loss: 0.000104\n",
      "Epoch 3584, Total Loss: 0.000051, PDE Loss: 0.000024, IC Loss: 0.000027\n",
      "Epoch 4096, Total Loss: 0.000094, PDE Loss: 0.000061, IC Loss: 0.000034\n",
      "Epoch 4608, Total Loss: 0.000030, PDE Loss: 0.000016, IC Loss: 0.000014\n",
      "Training completed in 391.73 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.084747, PDE Loss: 0.000023, IC Loss: 0.084724\n",
      "Epoch 512, Total Loss: 0.003965, PDE Loss: 0.002026, IC Loss: 0.001939\n",
      "Epoch 1024, Total Loss: 0.000273, PDE Loss: 0.000173, IC Loss: 0.000100\n",
      "Epoch 1536, Total Loss: 0.000970, PDE Loss: 0.000726, IC Loss: 0.000244\n",
      "Epoch 2048, Total Loss: 0.000146, PDE Loss: 0.000070, IC Loss: 0.000076\n",
      "Epoch 2560, Total Loss: 0.000345, PDE Loss: 0.000236, IC Loss: 0.000109\n",
      "Epoch 3072, Total Loss: 0.000112, PDE Loss: 0.000070, IC Loss: 0.000042\n",
      "Epoch 3584, Total Loss: 0.000052, PDE Loss: 0.000027, IC Loss: 0.000025\n",
      "Epoch 4096, Total Loss: 0.000058, PDE Loss: 0.000034, IC Loss: 0.000024\n",
      "Epoch 4608, Total Loss: 0.000047, PDE Loss: 0.000022, IC Loss: 0.000025\n",
      "Epoch 5120, Total Loss: 0.000036, PDE Loss: 0.000020, IC Loss: 0.000016\n",
      "Epoch 5632, Total Loss: 0.000054, PDE Loss: 0.000035, IC Loss: 0.000019\n",
      "Epoch 6144, Total Loss: 0.000192, PDE Loss: 0.000097, IC Loss: 0.000094\n",
      "Epoch 6656, Total Loss: 0.000053, PDE Loss: 0.000031, IC Loss: 0.000022\n",
      "Epoch 7168, Total Loss: 0.000014, PDE Loss: 0.000005, IC Loss: 0.000009\n",
      "Epoch 7680, Total Loss: 0.000045, PDE Loss: 0.000033, IC Loss: 0.000013\n",
      "Epoch 8192, Total Loss: 0.000054, PDE Loss: 0.000031, IC Loss: 0.000022\n",
      "Epoch 8704, Total Loss: 0.000086, PDE Loss: 0.000033, IC Loss: 0.000054\n",
      "Epoch 9216, Total Loss: 0.000019, PDE Loss: 0.000011, IC Loss: 0.000008\n",
      "Epoch 9728, Total Loss: 0.002534, PDE Loss: 0.001773, IC Loss: 0.000761\n",
      "Training completed in 407.31 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=128, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.050997, PDE Loss: 0.000002, IC Loss: 0.050995\n",
      "Epoch 512, Total Loss: 0.007342, PDE Loss: 0.003488, IC Loss: 0.003854\n",
      "Epoch 1024, Total Loss: 0.000905, PDE Loss: 0.000557, IC Loss: 0.000347\n",
      "Epoch 1536, Total Loss: 0.000183, PDE Loss: 0.000104, IC Loss: 0.000078\n",
      "Epoch 2048, Total Loss: 0.000117, PDE Loss: 0.000060, IC Loss: 0.000057\n",
      "Epoch 2560, Total Loss: 0.000099, PDE Loss: 0.000046, IC Loss: 0.000053\n",
      "Epoch 3072, Total Loss: 0.000069, PDE Loss: 0.000030, IC Loss: 0.000039\n",
      "Epoch 3584, Total Loss: 0.000061, PDE Loss: 0.000030, IC Loss: 0.000032\n",
      "Epoch 4096, Total Loss: 0.000044, PDE Loss: 0.000019, IC Loss: 0.000024\n",
      "Epoch 4608, Total Loss: 0.000099, PDE Loss: 0.000067, IC Loss: 0.000031\n",
      "Epoch 5120, Total Loss: 0.000047, PDE Loss: 0.000026, IC Loss: 0.000022\n",
      "Epoch 5632, Total Loss: 0.000066, PDE Loss: 0.000040, IC Loss: 0.000026\n",
      "Epoch 6144, Total Loss: 0.000057, PDE Loss: 0.000025, IC Loss: 0.000032\n",
      "Epoch 6656, Total Loss: 0.000027, PDE Loss: 0.000014, IC Loss: 0.000013\n",
      "Epoch 7168, Total Loss: 0.000072, PDE Loss: 0.000021, IC Loss: 0.000051\n",
      "Epoch 7680, Total Loss: 0.000198, PDE Loss: 0.000125, IC Loss: 0.000073\n",
      "Epoch 8192, Total Loss: 0.000034, PDE Loss: 0.000019, IC Loss: 0.000014\n",
      "Epoch 8704, Total Loss: 0.000020, PDE Loss: 0.000011, IC Loss: 0.000009\n",
      "Epoch 9216, Total Loss: 0.000467, PDE Loss: 0.000373, IC Loss: 0.000094\n",
      "Epoch 9728, Total Loss: 0.000010, PDE Loss: 0.000005, IC Loss: 0.000005\n",
      "Training completed in 782.92 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.064881, PDE Loss: 0.000015, IC Loss: 0.064865\n",
      "Epoch 512, Total Loss: 0.001165, PDE Loss: 0.000698, IC Loss: 0.000466\n",
      "Epoch 1024, Total Loss: 0.000183, PDE Loss: 0.000118, IC Loss: 0.000065\n",
      "Epoch 1536, Total Loss: 0.000114, PDE Loss: 0.000056, IC Loss: 0.000058\n",
      "Training completed in 205.32 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.043254, PDE Loss: 0.000004, IC Loss: 0.043250\n",
      "Epoch 512, Total Loss: 0.007038, PDE Loss: 0.003379, IC Loss: 0.003658\n",
      "Epoch 1024, Total Loss: 0.009608, PDE Loss: 0.005289, IC Loss: 0.004319\n",
      "Epoch 1536, Total Loss: 0.015363, PDE Loss: 0.000277, IC Loss: 0.015086\n",
      "Training completed in 406.34 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.074220, PDE Loss: 0.000000, IC Loss: 0.074220\n",
      "Epoch 512, Total Loss: 0.005829, PDE Loss: 0.002215, IC Loss: 0.003614\n",
      "Epoch 1024, Total Loss: 0.001927, PDE Loss: 0.001401, IC Loss: 0.000526\n",
      "Epoch 1536, Total Loss: 0.000117, PDE Loss: 0.000072, IC Loss: 0.000045\n",
      "Epoch 2048, Total Loss: 0.000163, PDE Loss: 0.000123, IC Loss: 0.000040\n",
      "Epoch 2560, Total Loss: 0.000395, PDE Loss: 0.000325, IC Loss: 0.000070\n",
      "Epoch 3072, Total Loss: 0.000303, PDE Loss: 0.000229, IC Loss: 0.000074\n",
      "Epoch 3584, Total Loss: 0.000125, PDE Loss: 0.000084, IC Loss: 0.000041\n",
      "Training completed in 411.96 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.066267, PDE Loss: 0.000020, IC Loss: 0.066247\n",
      "Epoch 512, Total Loss: 0.004841, PDE Loss: 0.001986, IC Loss: 0.002855\n",
      "Epoch 1024, Total Loss: 0.013413, PDE Loss: 0.007624, IC Loss: 0.005790\n",
      "Epoch 1536, Total Loss: 0.000550, PDE Loss: 0.000162, IC Loss: 0.000388\n",
      "Epoch 2048, Total Loss: 0.000614, PDE Loss: 0.000452, IC Loss: 0.000161\n",
      "Epoch 2560, Total Loss: 0.000052, PDE Loss: 0.000025, IC Loss: 0.000026\n",
      "Epoch 3072, Total Loss: 0.000040, PDE Loss: 0.000018, IC Loss: 0.000022\n",
      "Epoch 3584, Total Loss: 0.000039, PDE Loss: 0.000018, IC Loss: 0.000021\n",
      "Training completed in 819.01 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.044380, PDE Loss: 0.000027, IC Loss: 0.044353\n",
      "Epoch 512, Total Loss: 0.002279, PDE Loss: 0.001138, IC Loss: 0.001141\n",
      "Epoch 1024, Total Loss: 0.000855, PDE Loss: 0.000530, IC Loss: 0.000324\n",
      "Epoch 1536, Total Loss: 0.000113, PDE Loss: 0.000061, IC Loss: 0.000052\n",
      "Epoch 2048, Total Loss: 0.000075, PDE Loss: 0.000041, IC Loss: 0.000034\n",
      "Epoch 2560, Total Loss: 0.000052, PDE Loss: 0.000028, IC Loss: 0.000024\n",
      "Epoch 3072, Total Loss: 0.000042, PDE Loss: 0.000022, IC Loss: 0.000020\n",
      "Epoch 3584, Total Loss: 0.001129, PDE Loss: 0.000738, IC Loss: 0.000390\n",
      "Epoch 4096, Total Loss: 0.000041, PDE Loss: 0.000025, IC Loss: 0.000017\n",
      "Epoch 4608, Total Loss: 0.000159, PDE Loss: 0.000091, IC Loss: 0.000068\n",
      "Training completed in 517.59 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.065248, PDE Loss: 0.000055, IC Loss: 0.065193\n",
      "Epoch 512, Total Loss: 0.022206, PDE Loss: 0.001551, IC Loss: 0.020655\n",
      "Epoch 1024, Total Loss: 0.005520, PDE Loss: 0.002743, IC Loss: 0.002777\n",
      "Epoch 1536, Total Loss: 0.000696, PDE Loss: 0.000412, IC Loss: 0.000284\n",
      "Epoch 2048, Total Loss: 0.000153, PDE Loss: 0.000095, IC Loss: 0.000057\n",
      "Epoch 2560, Total Loss: 0.000046, PDE Loss: 0.000022, IC Loss: 0.000024\n",
      "Epoch 3072, Total Loss: 0.000039, PDE Loss: 0.000019, IC Loss: 0.000020\n",
      "Epoch 3584, Total Loss: 0.000026, PDE Loss: 0.000012, IC Loss: 0.000014\n",
      "Epoch 4096, Total Loss: 0.000104, PDE Loss: 0.000077, IC Loss: 0.000027\n",
      "Epoch 4608, Total Loss: 0.000070, PDE Loss: 0.000033, IC Loss: 0.000037\n",
      "Training completed in 1018.05 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.071981, PDE Loss: 0.000164, IC Loss: 0.071817\n",
      "Epoch 512, Total Loss: 0.009218, PDE Loss: 0.003976, IC Loss: 0.005242\n",
      "Epoch 1024, Total Loss: 0.001843, PDE Loss: 0.001126, IC Loss: 0.000716\n",
      "Epoch 1536, Total Loss: 0.000194, PDE Loss: 0.000126, IC Loss: 0.000068\n",
      "Epoch 2048, Total Loss: 0.000031, PDE Loss: 0.000022, IC Loss: 0.000009\n",
      "Epoch 2560, Total Loss: 0.000037, PDE Loss: 0.000025, IC Loss: 0.000012\n",
      "Epoch 3072, Total Loss: 0.000194, PDE Loss: 0.000143, IC Loss: 0.000051\n",
      "Epoch 3584, Total Loss: 0.000068, PDE Loss: 0.000054, IC Loss: 0.000014\n",
      "Epoch 4096, Total Loss: 0.000196, PDE Loss: 0.000148, IC Loss: 0.000048\n",
      "Epoch 4608, Total Loss: 0.000093, PDE Loss: 0.000058, IC Loss: 0.000035\n",
      "Epoch 5120, Total Loss: 0.000162, PDE Loss: 0.000128, IC Loss: 0.000033\n",
      "Epoch 5632, Total Loss: 0.000071, PDE Loss: 0.000039, IC Loss: 0.000032\n",
      "Epoch 6144, Total Loss: 0.012422, PDE Loss: 0.004446, IC Loss: 0.007976\n",
      "Epoch 6656, Total Loss: 0.000140, PDE Loss: 0.000110, IC Loss: 0.000030\n",
      "Epoch 7168, Total Loss: 0.000101, PDE Loss: 0.000086, IC Loss: 0.000014\n",
      "Epoch 7680, Total Loss: 0.000051, PDE Loss: 0.000040, IC Loss: 0.000012\n",
      "Epoch 8192, Total Loss: 0.000070, PDE Loss: 0.000054, IC Loss: 0.000016\n",
      "Epoch 8704, Total Loss: 0.000033, PDE Loss: 0.000030, IC Loss: 0.000004\n",
      "Epoch 9216, Total Loss: 0.006570, PDE Loss: 0.003853, IC Loss: 0.002717\n",
      "Epoch 9728, Total Loss: 0.000091, PDE Loss: 0.000074, IC Loss: 0.000017\n",
      "Training completed in 1029.46 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=6, neurons=256, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.056022, PDE Loss: 0.000013, IC Loss: 0.056009\n",
      "Epoch 512, Total Loss: 0.022775, PDE Loss: 0.000800, IC Loss: 0.021975\n",
      "Epoch 1024, Total Loss: 0.009993, PDE Loss: 0.007533, IC Loss: 0.002459\n",
      "Epoch 1536, Total Loss: 0.003285, PDE Loss: 0.001506, IC Loss: 0.001779\n",
      "Epoch 2048, Total Loss: 0.000654, PDE Loss: 0.000513, IC Loss: 0.000141\n",
      "Epoch 2560, Total Loss: 0.000299, PDE Loss: 0.000189, IC Loss: 0.000110\n",
      "Epoch 3072, Total Loss: 0.000163, PDE Loss: 0.000121, IC Loss: 0.000041\n",
      "Epoch 3584, Total Loss: 0.000038, PDE Loss: 0.000021, IC Loss: 0.000017\n",
      "Epoch 4096, Total Loss: 0.000022, PDE Loss: 0.000010, IC Loss: 0.000012\n",
      "Epoch 4608, Total Loss: 0.000064, PDE Loss: 0.000041, IC Loss: 0.000023\n",
      "Epoch 5120, Total Loss: 0.000040, PDE Loss: 0.000028, IC Loss: 0.000013\n",
      "Epoch 5632, Total Loss: 0.000023, PDE Loss: 0.000013, IC Loss: 0.000009\n",
      "Epoch 6144, Total Loss: 0.000039, PDE Loss: 0.000029, IC Loss: 0.000010\n",
      "Epoch 6656, Total Loss: 0.000274, PDE Loss: 0.000157, IC Loss: 0.000117\n",
      "Epoch 7168, Total Loss: 0.000003, PDE Loss: 0.000001, IC Loss: 0.000002\n",
      "Epoch 7680, Total Loss: 0.000026, PDE Loss: 0.000019, IC Loss: 0.000007\n",
      "Epoch 8192, Total Loss: 0.000570, PDE Loss: 0.000503, IC Loss: 0.000067\n",
      "Epoch 8704, Total Loss: 0.000079, PDE Loss: 0.000058, IC Loss: 0.000021\n",
      "Epoch 9216, Total Loss: 0.000020, PDE Loss: 0.000013, IC Loss: 0.000007\n",
      "Epoch 9728, Total Loss: 0.000052, PDE Loss: 0.000042, IC Loss: 0.000010\n",
      "Training completed in 2044.14 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.073188, PDE Loss: 0.000001, IC Loss: 0.073187\n",
      "Epoch 512, Total Loss: 0.008578, PDE Loss: 0.005330, IC Loss: 0.003248\n",
      "Epoch 1024, Total Loss: 0.002236, PDE Loss: 0.002053, IC Loss: 0.000183\n",
      "Epoch 1536, Total Loss: 0.000447, PDE Loss: 0.000129, IC Loss: 0.000318\n",
      "Training completed in 268.82 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.047881, PDE Loss: 0.000001, IC Loss: 0.047881\n",
      "Epoch 512, Total Loss: 0.000550, PDE Loss: 0.000387, IC Loss: 0.000163\n",
      "Epoch 1024, Total Loss: 0.000045, PDE Loss: 0.000020, IC Loss: 0.000024\n",
      "Epoch 1536, Total Loss: 0.000057, PDE Loss: 0.000030, IC Loss: 0.000027\n",
      "Training completed in 531.04 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.050824, PDE Loss: 0.000000, IC Loss: 0.050823\n",
      "Epoch 512, Total Loss: 0.000937, PDE Loss: 0.000630, IC Loss: 0.000307\n",
      "Epoch 1024, Total Loss: 0.000343, PDE Loss: 0.000154, IC Loss: 0.000189\n",
      "Epoch 1536, Total Loss: 0.000315, PDE Loss: 0.000155, IC Loss: 0.000160\n",
      "Epoch 2048, Total Loss: 0.000449, PDE Loss: 0.000206, IC Loss: 0.000242\n",
      "Epoch 2560, Total Loss: 0.000112, PDE Loss: 0.000066, IC Loss: 0.000046\n",
      "Epoch 3072, Total Loss: 0.000031, PDE Loss: 0.000018, IC Loss: 0.000012\n",
      "Epoch 3584, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Training completed in 538.32 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.043115, PDE Loss: 0.000000, IC Loss: 0.043115\n",
      "Epoch 512, Total Loss: 0.000213, PDE Loss: 0.000118, IC Loss: 0.000095\n",
      "Epoch 1024, Total Loss: 0.000128, PDE Loss: 0.000066, IC Loss: 0.000062\n",
      "Epoch 1536, Total Loss: 0.000125, PDE Loss: 0.000062, IC Loss: 0.000063\n",
      "Epoch 2048, Total Loss: 0.000138, PDE Loss: 0.000080, IC Loss: 0.000058\n",
      "Epoch 2560, Total Loss: 0.000133, PDE Loss: 0.000059, IC Loss: 0.000074\n",
      "Epoch 3072, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035443\n",
      "Epoch 3584, Total Loss: 0.035442, PDE Loss: 0.000001, IC Loss: 0.035441\n",
      "Training completed in 1063.70 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.063718, PDE Loss: 0.000000, IC Loss: 0.063718\n",
      "Epoch 512, Total Loss: 0.000793, PDE Loss: 0.000504, IC Loss: 0.000289\n",
      "Epoch 1024, Total Loss: 0.000067, PDE Loss: 0.000038, IC Loss: 0.000029\n",
      "Epoch 1536, Total Loss: 0.000366, PDE Loss: 0.000212, IC Loss: 0.000154\n",
      "Epoch 2048, Total Loss: 0.000128, PDE Loss: 0.000104, IC Loss: 0.000024\n",
      "Epoch 2560, Total Loss: 0.000167, PDE Loss: 0.000112, IC Loss: 0.000055\n",
      "Epoch 3072, Total Loss: 0.011314, PDE Loss: 0.004296, IC Loss: 0.007018\n",
      "Epoch 3584, Total Loss: 0.003373, PDE Loss: 0.001561, IC Loss: 0.001812\n",
      "Epoch 4096, Total Loss: 0.000015, PDE Loss: 0.000007, IC Loss: 0.000008\n",
      "Epoch 4608, Total Loss: 0.000170, PDE Loss: 0.000076, IC Loss: 0.000094\n",
      "Training completed in 673.35 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.073913, PDE Loss: 0.000001, IC Loss: 0.073912\n",
      "Epoch 512, Total Loss: 0.002918, PDE Loss: 0.001716, IC Loss: 0.001202\n",
      "Epoch 1024, Total Loss: 0.000115, PDE Loss: 0.000051, IC Loss: 0.000064\n",
      "Epoch 1536, Total Loss: 0.000290, PDE Loss: 0.000223, IC Loss: 0.000067\n",
      "Epoch 2048, Total Loss: 0.000332, PDE Loss: 0.000189, IC Loss: 0.000143\n",
      "Epoch 2560, Total Loss: 0.000031, PDE Loss: 0.000021, IC Loss: 0.000010\n",
      "Epoch 3072, Total Loss: 0.000047, PDE Loss: 0.000030, IC Loss: 0.000017\n",
      "Epoch 3584, Total Loss: 0.000090, PDE Loss: 0.000060, IC Loss: 0.000030\n",
      "Epoch 4096, Total Loss: 0.000087, PDE Loss: 0.000055, IC Loss: 0.000031\n",
      "Epoch 4608, Total Loss: 0.000026, PDE Loss: 0.000015, IC Loss: 0.000011\n",
      "Training completed in 1331.87 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.066067, PDE Loss: 0.000000, IC Loss: 0.066067\n",
      "Epoch 512, Total Loss: 0.000384, PDE Loss: 0.000206, IC Loss: 0.000178\n",
      "Epoch 1024, Total Loss: 0.000104, PDE Loss: 0.000041, IC Loss: 0.000063\n",
      "Epoch 1536, Total Loss: 0.000065, PDE Loss: 0.000030, IC Loss: 0.000036\n",
      "Epoch 2048, Total Loss: 0.000047, PDE Loss: 0.000023, IC Loss: 0.000023\n",
      "Epoch 2560, Total Loss: 0.000048, PDE Loss: 0.000028, IC Loss: 0.000021\n",
      "Epoch 3072, Total Loss: 0.035290, PDE Loss: 0.000001, IC Loss: 0.035289\n",
      "Epoch 3584, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 4096, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 4608, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 5120, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 5632, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 6144, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 6656, Total Loss: 0.035443, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 7168, Total Loss: 0.035439, PDE Loss: 0.000001, IC Loss: 0.035438\n",
      "Epoch 7680, Total Loss: 0.035443, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 8192, Total Loss: 0.035443, PDE Loss: 0.000001, IC Loss: 0.035441\n",
      "Epoch 8704, Total Loss: 0.027267, PDE Loss: 0.000943, IC Loss: 0.026324\n",
      "Epoch 9216, Total Loss: 0.027240, PDE Loss: 0.000930, IC Loss: 0.026309\n",
      "Epoch 9728, Total Loss: 0.027247, PDE Loss: 0.000932, IC Loss: 0.026315\n",
      "Training completed in 1339.99 seconds.\n",
      "\n",
      "Running experiment: activation=tanh, hidden_layers=8, neurons=256, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.080009, PDE Loss: 0.000000, IC Loss: 0.080009\n",
      "Epoch 512, Total Loss: 0.000330, PDE Loss: 0.000190, IC Loss: 0.000139\n",
      "Epoch 1024, Total Loss: 0.000132, PDE Loss: 0.000068, IC Loss: 0.000064\n",
      "Epoch 1536, Total Loss: 0.000156, PDE Loss: 0.000107, IC Loss: 0.000049\n",
      "Epoch 2048, Total Loss: 0.000047, PDE Loss: 0.000020, IC Loss: 0.000028\n",
      "Epoch 2560, Total Loss: 0.035443, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 3072, Total Loss: 0.035442, PDE Loss: 0.000001, IC Loss: 0.035441\n",
      "Epoch 3584, Total Loss: 0.035441, PDE Loss: 0.000001, IC Loss: 0.035440\n",
      "Epoch 4096, Total Loss: 0.035434, PDE Loss: 0.000001, IC Loss: 0.035433\n",
      "Epoch 4608, Total Loss: 0.035445, PDE Loss: 0.000001, IC Loss: 0.035443\n",
      "Epoch 5120, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035443\n",
      "Epoch 5632, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035443\n",
      "Epoch 6144, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 6656, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 7168, Total Loss: 0.035443, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 7680, Total Loss: 0.035406, PDE Loss: 0.000001, IC Loss: 0.035405\n",
      "Epoch 8192, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 8704, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Epoch 9216, Total Loss: 0.035443, PDE Loss: 0.000001, IC Loss: 0.035441\n",
      "Epoch 9728, Total Loss: 0.035444, PDE Loss: 0.000001, IC Loss: 0.035442\n",
      "Training completed in 2645.91 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.106106, PDE Loss: 0.000484, IC Loss: 0.105622\n",
      "Epoch 512, Total Loss: 0.002451, PDE Loss: 0.001262, IC Loss: 0.001189\n",
      "Epoch 1024, Total Loss: 0.001071, PDE Loss: 0.000652, IC Loss: 0.000418\n",
      "Epoch 1536, Total Loss: 0.000812, PDE Loss: 0.000343, IC Loss: 0.000469\n",
      "Training completed in 37.93 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.079982, PDE Loss: 0.000097, IC Loss: 0.079885\n",
      "Epoch 512, Total Loss: 0.000508, PDE Loss: 0.000318, IC Loss: 0.000190\n",
      "Epoch 1024, Total Loss: 0.000196, PDE Loss: 0.000098, IC Loss: 0.000098\n",
      "Epoch 1536, Total Loss: 0.000111, PDE Loss: 0.000039, IC Loss: 0.000072\n",
      "Training completed in 71.29 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.061169, PDE Loss: 0.000362, IC Loss: 0.060807\n",
      "Epoch 512, Total Loss: 0.000459, PDE Loss: 0.000246, IC Loss: 0.000213\n",
      "Epoch 1024, Total Loss: 0.000172, PDE Loss: 0.000087, IC Loss: 0.000085\n",
      "Epoch 1536, Total Loss: 0.000174, PDE Loss: 0.000043, IC Loss: 0.000131\n",
      "Epoch 2048, Total Loss: 0.000092, PDE Loss: 0.000041, IC Loss: 0.000051\n",
      "Epoch 2560, Total Loss: 0.000074, PDE Loss: 0.000032, IC Loss: 0.000042\n",
      "Epoch 3072, Total Loss: 0.000064, PDE Loss: 0.000027, IC Loss: 0.000037\n",
      "Epoch 3584, Total Loss: 0.000095, PDE Loss: 0.000051, IC Loss: 0.000045\n",
      "Training completed in 75.93 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.040134, PDE Loss: 0.001076, IC Loss: 0.039059\n",
      "Epoch 512, Total Loss: 0.001230, PDE Loss: 0.000671, IC Loss: 0.000560\n",
      "Epoch 1024, Total Loss: 0.000683, PDE Loss: 0.000195, IC Loss: 0.000488\n",
      "Epoch 1536, Total Loss: 0.000279, PDE Loss: 0.000146, IC Loss: 0.000133\n",
      "Epoch 2048, Total Loss: 0.000195, PDE Loss: 0.000098, IC Loss: 0.000098\n",
      "Epoch 2560, Total Loss: 0.000154, PDE Loss: 0.000056, IC Loss: 0.000099\n",
      "Epoch 3072, Total Loss: 0.000093, PDE Loss: 0.000036, IC Loss: 0.000057\n",
      "Epoch 3584, Total Loss: 0.000072, PDE Loss: 0.000026, IC Loss: 0.000045\n",
      "Training completed in 142.42 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.042898, PDE Loss: 0.000204, IC Loss: 0.042694\n",
      "Epoch 512, Total Loss: 0.002100, PDE Loss: 0.000937, IC Loss: 0.001164\n",
      "Epoch 1024, Total Loss: 0.000665, PDE Loss: 0.000236, IC Loss: 0.000429\n",
      "Epoch 1536, Total Loss: 0.000395, PDE Loss: 0.000250, IC Loss: 0.000145\n",
      "Epoch 2048, Total Loss: 0.000140, PDE Loss: 0.000065, IC Loss: 0.000075\n",
      "Epoch 2560, Total Loss: 0.000309, PDE Loss: 0.000226, IC Loss: 0.000083\n",
      "Epoch 3072, Total Loss: 0.000129, PDE Loss: 0.000071, IC Loss: 0.000059\n",
      "Epoch 3584, Total Loss: 0.000083, PDE Loss: 0.000038, IC Loss: 0.000045\n",
      "Epoch 4096, Total Loss: 0.000076, PDE Loss: 0.000032, IC Loss: 0.000044\n",
      "Epoch 4608, Total Loss: 0.000055, PDE Loss: 0.000023, IC Loss: 0.000032\n",
      "Training completed in 94.99 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.098367, PDE Loss: 0.000422, IC Loss: 0.097946\n",
      "Epoch 512, Total Loss: 0.000855, PDE Loss: 0.000391, IC Loss: 0.000464\n",
      "Epoch 1024, Total Loss: 0.000263, PDE Loss: 0.000121, IC Loss: 0.000142\n",
      "Epoch 1536, Total Loss: 0.000352, PDE Loss: 0.000172, IC Loss: 0.000180\n",
      "Epoch 2048, Total Loss: 0.000110, PDE Loss: 0.000041, IC Loss: 0.000070\n",
      "Epoch 2560, Total Loss: 0.000161, PDE Loss: 0.000090, IC Loss: 0.000072\n",
      "Epoch 3072, Total Loss: 0.000066, PDE Loss: 0.000017, IC Loss: 0.000049\n",
      "Epoch 3584, Total Loss: 0.000056, PDE Loss: 0.000012, IC Loss: 0.000044\n",
      "Epoch 4096, Total Loss: 0.000051, PDE Loss: 0.000011, IC Loss: 0.000039\n",
      "Epoch 4608, Total Loss: 0.000046, PDE Loss: 0.000010, IC Loss: 0.000036\n",
      "Training completed in 178.64 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.052513, PDE Loss: 0.000018, IC Loss: 0.052495\n",
      "Epoch 512, Total Loss: 0.000643, PDE Loss: 0.000322, IC Loss: 0.000321\n",
      "Epoch 1024, Total Loss: 0.000595, PDE Loss: 0.000409, IC Loss: 0.000186\n",
      "Epoch 1536, Total Loss: 0.000251, PDE Loss: 0.000084, IC Loss: 0.000166\n",
      "Epoch 2048, Total Loss: 0.000187, PDE Loss: 0.000056, IC Loss: 0.000131\n",
      "Epoch 2560, Total Loss: 0.000125, PDE Loss: 0.000041, IC Loss: 0.000085\n",
      "Epoch 3072, Total Loss: 0.000102, PDE Loss: 0.000030, IC Loss: 0.000072\n",
      "Epoch 3584, Total Loss: 0.000088, PDE Loss: 0.000027, IC Loss: 0.000061\n",
      "Epoch 4096, Total Loss: 0.000075, PDE Loss: 0.000023, IC Loss: 0.000051\n",
      "Epoch 4608, Total Loss: 0.000064, PDE Loss: 0.000021, IC Loss: 0.000043\n",
      "Epoch 5120, Total Loss: 0.000057, PDE Loss: 0.000020, IC Loss: 0.000037\n",
      "Epoch 5632, Total Loss: 0.000051, PDE Loss: 0.000018, IC Loss: 0.000033\n",
      "Epoch 6144, Total Loss: 0.000136, PDE Loss: 0.000061, IC Loss: 0.000075\n",
      "Epoch 6656, Total Loss: 0.000136, PDE Loss: 0.000092, IC Loss: 0.000044\n",
      "Epoch 7168, Total Loss: 0.000053, PDE Loss: 0.000018, IC Loss: 0.000035\n",
      "Epoch 7680, Total Loss: 0.000032, PDE Loss: 0.000009, IC Loss: 0.000023\n",
      "Epoch 8192, Total Loss: 0.000032, PDE Loss: 0.000010, IC Loss: 0.000022\n",
      "Epoch 8704, Total Loss: 0.000030, PDE Loss: 0.000010, IC Loss: 0.000020\n",
      "Epoch 9216, Total Loss: 0.000179, PDE Loss: 0.000108, IC Loss: 0.000071\n",
      "Epoch 9728, Total Loss: 0.000049, PDE Loss: 0.000023, IC Loss: 0.000026\n",
      "Training completed in 189.69 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=64, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.035406, PDE Loss: 0.000352, IC Loss: 0.035054\n",
      "Epoch 512, Total Loss: 0.000995, PDE Loss: 0.000515, IC Loss: 0.000481\n",
      "Epoch 1024, Total Loss: 0.000230, PDE Loss: 0.000117, IC Loss: 0.000112\n",
      "Epoch 1536, Total Loss: 0.000133, PDE Loss: 0.000054, IC Loss: 0.000079\n",
      "Epoch 2048, Total Loss: 0.000104, PDE Loss: 0.000042, IC Loss: 0.000062\n",
      "Epoch 2560, Total Loss: 0.000092, PDE Loss: 0.000039, IC Loss: 0.000053\n",
      "Epoch 3072, Total Loss: 0.000085, PDE Loss: 0.000037, IC Loss: 0.000048\n",
      "Epoch 3584, Total Loss: 0.000079, PDE Loss: 0.000036, IC Loss: 0.000044\n",
      "Epoch 4096, Total Loss: 0.000071, PDE Loss: 0.000031, IC Loss: 0.000040\n",
      "Epoch 4608, Total Loss: 0.000090, PDE Loss: 0.000044, IC Loss: 0.000046\n",
      "Epoch 5120, Total Loss: 0.000069, PDE Loss: 0.000028, IC Loss: 0.000041\n",
      "Epoch 5632, Total Loss: 0.000050, PDE Loss: 0.000018, IC Loss: 0.000031\n",
      "Epoch 6144, Total Loss: 0.000045, PDE Loss: 0.000016, IC Loss: 0.000029\n",
      "Epoch 6656, Total Loss: 0.000094, PDE Loss: 0.000055, IC Loss: 0.000039\n",
      "Epoch 7168, Total Loss: 0.000083, PDE Loss: 0.000042, IC Loss: 0.000041\n",
      "Epoch 7680, Total Loss: 0.000037, PDE Loss: 0.000015, IC Loss: 0.000021\n",
      "Epoch 8192, Total Loss: 0.000029, PDE Loss: 0.000012, IC Loss: 0.000017\n",
      "Epoch 8704, Total Loss: 0.000021, PDE Loss: 0.000009, IC Loss: 0.000012\n",
      "Epoch 9216, Total Loss: 0.000023, PDE Loss: 0.000013, IC Loss: 0.000010\n",
      "Epoch 9728, Total Loss: 0.000020, PDE Loss: 0.000011, IC Loss: 0.000009\n",
      "Training completed in 359.75 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.055108, PDE Loss: 0.000026, IC Loss: 0.055083\n",
      "Epoch 512, Total Loss: 0.002810, PDE Loss: 0.001123, IC Loss: 0.001687\n",
      "Epoch 1024, Total Loss: 0.000827, PDE Loss: 0.000520, IC Loss: 0.000307\n",
      "Epoch 1536, Total Loss: 0.000402, PDE Loss: 0.000232, IC Loss: 0.000169\n",
      "Training completed in 66.92 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.043975, PDE Loss: 0.000162, IC Loss: 0.043813\n",
      "Epoch 512, Total Loss: 0.003206, PDE Loss: 0.001622, IC Loss: 0.001584\n",
      "Epoch 1024, Total Loss: 0.001612, PDE Loss: 0.000815, IC Loss: 0.000798\n",
      "Epoch 1536, Total Loss: 0.000702, PDE Loss: 0.000443, IC Loss: 0.000259\n",
      "Training completed in 127.16 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.072505, PDE Loss: 0.000267, IC Loss: 0.072239\n",
      "Epoch 512, Total Loss: 0.000566, PDE Loss: 0.000279, IC Loss: 0.000287\n",
      "Epoch 1024, Total Loss: 0.000185, PDE Loss: 0.000089, IC Loss: 0.000096\n",
      "Epoch 1536, Total Loss: 0.000130, PDE Loss: 0.000049, IC Loss: 0.000081\n",
      "Epoch 2048, Total Loss: 0.000152, PDE Loss: 0.000057, IC Loss: 0.000094\n",
      "Epoch 2560, Total Loss: 0.000111, PDE Loss: 0.000032, IC Loss: 0.000080\n",
      "Epoch 3072, Total Loss: 0.000085, PDE Loss: 0.000031, IC Loss: 0.000055\n",
      "Epoch 3584, Total Loss: 0.005672, PDE Loss: 0.001835, IC Loss: 0.003837\n",
      "Training completed in 132.08 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.059348, PDE Loss: 0.000201, IC Loss: 0.059147\n",
      "Epoch 512, Total Loss: 0.004350, PDE Loss: 0.002753, IC Loss: 0.001597\n",
      "Epoch 1024, Total Loss: 0.000866, PDE Loss: 0.000505, IC Loss: 0.000361\n",
      "Epoch 1536, Total Loss: 0.000296, PDE Loss: 0.000161, IC Loss: 0.000135\n",
      "Epoch 2048, Total Loss: 0.000205, PDE Loss: 0.000088, IC Loss: 0.000117\n",
      "Epoch 2560, Total Loss: 0.000154, PDE Loss: 0.000062, IC Loss: 0.000092\n",
      "Epoch 3072, Total Loss: 0.000182, PDE Loss: 0.000060, IC Loss: 0.000121\n",
      "Epoch 3584, Total Loss: 0.000449, PDE Loss: 0.000335, IC Loss: 0.000114\n",
      "Training completed in 253.58 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.065132, PDE Loss: 0.000506, IC Loss: 0.064626\n",
      "Epoch 512, Total Loss: 0.001870, PDE Loss: 0.001032, IC Loss: 0.000838\n",
      "Epoch 1024, Total Loss: 0.000284, PDE Loss: 0.000158, IC Loss: 0.000126\n",
      "Epoch 1536, Total Loss: 0.000552, PDE Loss: 0.000355, IC Loss: 0.000197\n",
      "Epoch 2048, Total Loss: 0.000077, PDE Loss: 0.000020, IC Loss: 0.000056\n",
      "Epoch 2560, Total Loss: 0.000058, PDE Loss: 0.000017, IC Loss: 0.000041\n",
      "Epoch 3072, Total Loss: 0.000047, PDE Loss: 0.000012, IC Loss: 0.000035\n",
      "Epoch 3584, Total Loss: 0.000052, PDE Loss: 0.000020, IC Loss: 0.000032\n",
      "Epoch 4096, Total Loss: 0.000116, PDE Loss: 0.000081, IC Loss: 0.000036\n",
      "Epoch 4608, Total Loss: 0.000109, PDE Loss: 0.000070, IC Loss: 0.000039\n",
      "Training completed in 165.06 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.086521, PDE Loss: 0.000269, IC Loss: 0.086251\n",
      "Epoch 512, Total Loss: 0.003138, PDE Loss: 0.001549, IC Loss: 0.001589\n",
      "Epoch 1024, Total Loss: 0.000745, PDE Loss: 0.000407, IC Loss: 0.000339\n",
      "Epoch 1536, Total Loss: 0.000238, PDE Loss: 0.000105, IC Loss: 0.000133\n",
      "Epoch 2048, Total Loss: 0.000131, PDE Loss: 0.000032, IC Loss: 0.000099\n",
      "Epoch 2560, Total Loss: 0.000099, PDE Loss: 0.000021, IC Loss: 0.000078\n",
      "Epoch 3072, Total Loss: 0.001212, PDE Loss: 0.000775, IC Loss: 0.000437\n",
      "Epoch 3584, Total Loss: 0.000090, PDE Loss: 0.000035, IC Loss: 0.000054\n",
      "Epoch 4096, Total Loss: 0.000050, PDE Loss: 0.000012, IC Loss: 0.000038\n",
      "Epoch 4608, Total Loss: 0.000551, PDE Loss: 0.000356, IC Loss: 0.000195\n",
      "Training completed in 316.80 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.112576, PDE Loss: 0.000275, IC Loss: 0.112301\n",
      "Epoch 512, Total Loss: 0.002043, PDE Loss: 0.001142, IC Loss: 0.000900\n",
      "Epoch 1024, Total Loss: 0.000920, PDE Loss: 0.000430, IC Loss: 0.000490\n",
      "Epoch 1536, Total Loss: 0.000304, PDE Loss: 0.000169, IC Loss: 0.000134\n",
      "Epoch 2048, Total Loss: 0.000183, PDE Loss: 0.000092, IC Loss: 0.000091\n",
      "Epoch 2560, Total Loss: 0.000134, PDE Loss: 0.000057, IC Loss: 0.000077\n",
      "Epoch 3072, Total Loss: 0.000107, PDE Loss: 0.000040, IC Loss: 0.000067\n",
      "Epoch 3584, Total Loss: 0.000101, PDE Loss: 0.000043, IC Loss: 0.000059\n",
      "Epoch 4096, Total Loss: 0.000643, PDE Loss: 0.000388, IC Loss: 0.000256\n",
      "Epoch 4608, Total Loss: 0.000138, PDE Loss: 0.000080, IC Loss: 0.000058\n",
      "Epoch 5120, Total Loss: 0.000069, PDE Loss: 0.000025, IC Loss: 0.000044\n",
      "Epoch 5632, Total Loss: 0.000061, PDE Loss: 0.000021, IC Loss: 0.000040\n",
      "Epoch 6144, Total Loss: 0.000055, PDE Loss: 0.000017, IC Loss: 0.000038\n",
      "Epoch 6656, Total Loss: 0.000053, PDE Loss: 0.000017, IC Loss: 0.000036\n",
      "Epoch 7168, Total Loss: 0.000098, PDE Loss: 0.000047, IC Loss: 0.000052\n",
      "Epoch 7680, Total Loss: 0.000126, PDE Loss: 0.000075, IC Loss: 0.000051\n",
      "Epoch 8192, Total Loss: 0.000048, PDE Loss: 0.000017, IC Loss: 0.000031\n",
      "Epoch 8704, Total Loss: 0.000499, PDE Loss: 0.000234, IC Loss: 0.000266\n",
      "Epoch 9216, Total Loss: 0.000045, PDE Loss: 0.000014, IC Loss: 0.000032\n",
      "Epoch 9728, Total Loss: 0.000035, PDE Loss: 0.000008, IC Loss: 0.000027\n",
      "Training completed in 330.23 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=4, neurons=128, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.063038, PDE Loss: 0.000626, IC Loss: 0.062412\n",
      "Epoch 512, Total Loss: 0.001907, PDE Loss: 0.001031, IC Loss: 0.000876\n",
      "Epoch 1024, Total Loss: 0.001163, PDE Loss: 0.000521, IC Loss: 0.000642\n",
      "Epoch 1536, Total Loss: 0.000208, PDE Loss: 0.000090, IC Loss: 0.000119\n",
      "Epoch 2048, Total Loss: 0.000140, PDE Loss: 0.000047, IC Loss: 0.000092\n",
      "Epoch 2560, Total Loss: 0.000088, PDE Loss: 0.000027, IC Loss: 0.000061\n",
      "Epoch 3072, Total Loss: 0.000069, PDE Loss: 0.000024, IC Loss: 0.000045\n",
      "Epoch 3584, Total Loss: 0.000060, PDE Loss: 0.000022, IC Loss: 0.000038\n",
      "Epoch 4096, Total Loss: 0.000338, PDE Loss: 0.000254, IC Loss: 0.000084\n",
      "Epoch 4608, Total Loss: 0.000048, PDE Loss: 0.000016, IC Loss: 0.000032\n",
      "Epoch 5120, Total Loss: 0.000046, PDE Loss: 0.000015, IC Loss: 0.000031\n",
      "Epoch 5632, Total Loss: 0.000039, PDE Loss: 0.000012, IC Loss: 0.000028\n",
      "Epoch 6144, Total Loss: 0.000037, PDE Loss: 0.000012, IC Loss: 0.000025\n",
      "Epoch 6656, Total Loss: 0.000041, PDE Loss: 0.000016, IC Loss: 0.000025\n",
      "Epoch 7168, Total Loss: 0.000032, PDE Loss: 0.000012, IC Loss: 0.000020\n",
      "Epoch 7680, Total Loss: 0.000031, PDE Loss: 0.000012, IC Loss: 0.000019\n",
      "Epoch 8192, Total Loss: 0.000061, PDE Loss: 0.000029, IC Loss: 0.000033\n",
      "Epoch 8704, Total Loss: 0.000037, PDE Loss: 0.000013, IC Loss: 0.000024\n",
      "Epoch 9216, Total Loss: 0.000030, PDE Loss: 0.000010, IC Loss: 0.000020\n",
      "Epoch 9728, Total Loss: 0.000026, PDE Loss: 0.000008, IC Loss: 0.000018\n",
      "Training completed in 634.31 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.053027, PDE Loss: 0.000008, IC Loss: 0.053019\n",
      "Epoch 512, Total Loss: 0.001972, PDE Loss: 0.001759, IC Loss: 0.000213\n",
      "Epoch 1024, Total Loss: 0.000142, PDE Loss: 0.000076, IC Loss: 0.000066\n",
      "Epoch 1536, Total Loss: 0.000172, PDE Loss: 0.000112, IC Loss: 0.000060\n",
      "Training completed in 92.04 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.042433, PDE Loss: 0.000006, IC Loss: 0.042427\n",
      "Epoch 512, Total Loss: 0.001395, PDE Loss: 0.000833, IC Loss: 0.000562\n",
      "Epoch 1024, Total Loss: 0.000690, PDE Loss: 0.000462, IC Loss: 0.000228\n",
      "Epoch 1536, Total Loss: 0.000290, PDE Loss: 0.000177, IC Loss: 0.000113\n",
      "Training completed in 177.37 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.091557, PDE Loss: 0.000003, IC Loss: 0.091554\n",
      "Epoch 512, Total Loss: 0.000781, PDE Loss: 0.000421, IC Loss: 0.000361\n",
      "Epoch 1024, Total Loss: 0.000102, PDE Loss: 0.000048, IC Loss: 0.000054\n",
      "Epoch 1536, Total Loss: 0.000119, PDE Loss: 0.000064, IC Loss: 0.000055\n",
      "Epoch 2048, Total Loss: 0.000072, PDE Loss: 0.000028, IC Loss: 0.000044\n",
      "Epoch 2560, Total Loss: 0.000051, PDE Loss: 0.000019, IC Loss: 0.000032\n",
      "Epoch 3072, Total Loss: 0.000046, PDE Loss: 0.000015, IC Loss: 0.000031\n",
      "Epoch 3584, Total Loss: 0.000041, PDE Loss: 0.000014, IC Loss: 0.000027\n",
      "Training completed in 183.98 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.080173, PDE Loss: 0.000042, IC Loss: 0.080131\n",
      "Epoch 512, Total Loss: 0.001620, PDE Loss: 0.000962, IC Loss: 0.000658\n",
      "Epoch 1024, Total Loss: 0.000114, PDE Loss: 0.000056, IC Loss: 0.000058\n",
      "Epoch 1536, Total Loss: 0.000070, PDE Loss: 0.000028, IC Loss: 0.000043\n",
      "Epoch 2048, Total Loss: 0.000071, PDE Loss: 0.000026, IC Loss: 0.000045\n",
      "Epoch 2560, Total Loss: 0.000054, PDE Loss: 0.000018, IC Loss: 0.000036\n",
      "Epoch 3072, Total Loss: 0.000220, PDE Loss: 0.000089, IC Loss: 0.000130\n",
      "Epoch 3584, Total Loss: 0.000046, PDE Loss: 0.000014, IC Loss: 0.000032\n",
      "Training completed in 355.11 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.057293, PDE Loss: 0.000019, IC Loss: 0.057273\n",
      "Epoch 512, Total Loss: 0.002376, PDE Loss: 0.001208, IC Loss: 0.001168\n",
      "Epoch 1024, Total Loss: 0.000320, PDE Loss: 0.000122, IC Loss: 0.000198\n",
      "Epoch 1536, Total Loss: 0.000125, PDE Loss: 0.000049, IC Loss: 0.000076\n",
      "Epoch 2048, Total Loss: 0.000083, PDE Loss: 0.000025, IC Loss: 0.000058\n",
      "Epoch 2560, Total Loss: 0.000102, PDE Loss: 0.000055, IC Loss: 0.000048\n",
      "Epoch 3072, Total Loss: 0.000068, PDE Loss: 0.000031, IC Loss: 0.000037\n",
      "Epoch 3584, Total Loss: 0.000050, PDE Loss: 0.000018, IC Loss: 0.000032\n",
      "Epoch 4096, Total Loss: 0.000176, PDE Loss: 0.000128, IC Loss: 0.000048\n",
      "Epoch 4608, Total Loss: 0.000049, PDE Loss: 0.000019, IC Loss: 0.000030\n",
      "Training completed in 230.21 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.041201, PDE Loss: 0.000002, IC Loss: 0.041199\n",
      "Epoch 512, Total Loss: 0.001356, PDE Loss: 0.000889, IC Loss: 0.000467\n",
      "Epoch 1024, Total Loss: 0.000664, PDE Loss: 0.000391, IC Loss: 0.000273\n",
      "Epoch 1536, Total Loss: 0.000448, PDE Loss: 0.000289, IC Loss: 0.000159\n",
      "Epoch 2048, Total Loss: 0.000272, PDE Loss: 0.000134, IC Loss: 0.000138\n",
      "Epoch 2560, Total Loss: 0.000446, PDE Loss: 0.000328, IC Loss: 0.000118\n",
      "Epoch 3072, Total Loss: 0.000103, PDE Loss: 0.000044, IC Loss: 0.000059\n",
      "Epoch 3584, Total Loss: 0.000075, PDE Loss: 0.000025, IC Loss: 0.000050\n",
      "Epoch 4096, Total Loss: 0.000059, PDE Loss: 0.000019, IC Loss: 0.000040\n",
      "Epoch 4608, Total Loss: 0.000057, PDE Loss: 0.000018, IC Loss: 0.000040\n",
      "Training completed in 443.77 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.082628, PDE Loss: 0.000111, IC Loss: 0.082516\n",
      "Epoch 512, Total Loss: 0.000483, PDE Loss: 0.000293, IC Loss: 0.000190\n",
      "Epoch 1024, Total Loss: 0.000278, PDE Loss: 0.000141, IC Loss: 0.000136\n",
      "Epoch 1536, Total Loss: 0.000195, PDE Loss: 0.000094, IC Loss: 0.000101\n",
      "Epoch 2048, Total Loss: 0.000097, PDE Loss: 0.000038, IC Loss: 0.000059\n",
      "Epoch 2560, Total Loss: 0.000107, PDE Loss: 0.000060, IC Loss: 0.000047\n",
      "Epoch 3072, Total Loss: 0.000169, PDE Loss: 0.000112, IC Loss: 0.000056\n",
      "Epoch 3584, Total Loss: 0.000228, PDE Loss: 0.000100, IC Loss: 0.000127\n",
      "Epoch 4096, Total Loss: 0.000158, PDE Loss: 0.000069, IC Loss: 0.000089\n",
      "Epoch 4608, Total Loss: 0.007189, PDE Loss: 0.003071, IC Loss: 0.004118\n",
      "Epoch 5120, Total Loss: 0.000116, PDE Loss: 0.000054, IC Loss: 0.000062\n",
      "Epoch 5632, Total Loss: 0.000170, PDE Loss: 0.000118, IC Loss: 0.000052\n",
      "Epoch 6144, Total Loss: 0.000039, PDE Loss: 0.000015, IC Loss: 0.000024\n",
      "Epoch 6656, Total Loss: 0.000028, PDE Loss: 0.000010, IC Loss: 0.000018\n",
      "Epoch 7168, Total Loss: 0.000044, PDE Loss: 0.000017, IC Loss: 0.000027\n",
      "Epoch 7680, Total Loss: 0.000063, PDE Loss: 0.000046, IC Loss: 0.000018\n",
      "Epoch 8192, Total Loss: 0.000020, PDE Loss: 0.000008, IC Loss: 0.000012\n",
      "Epoch 8704, Total Loss: 0.000040, PDE Loss: 0.000020, IC Loss: 0.000020\n",
      "Epoch 9216, Total Loss: 0.000031, PDE Loss: 0.000017, IC Loss: 0.000015\n",
      "Epoch 9728, Total Loss: 0.000039, PDE Loss: 0.000022, IC Loss: 0.000017\n",
      "Training completed in 460.12 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.068898, PDE Loss: 0.000014, IC Loss: 0.068884\n",
      "Epoch 512, Total Loss: 0.000429, PDE Loss: 0.000282, IC Loss: 0.000147\n",
      "Epoch 1024, Total Loss: 0.000144, PDE Loss: 0.000075, IC Loss: 0.000069\n",
      "Epoch 1536, Total Loss: 0.000120, PDE Loss: 0.000069, IC Loss: 0.000051\n",
      "Epoch 2048, Total Loss: 0.000122, PDE Loss: 0.000075, IC Loss: 0.000048\n",
      "Epoch 2560, Total Loss: 0.000090, PDE Loss: 0.000051, IC Loss: 0.000039\n",
      "Epoch 3072, Total Loss: 0.000066, PDE Loss: 0.000033, IC Loss: 0.000033\n",
      "Epoch 3584, Total Loss: 0.000060, PDE Loss: 0.000027, IC Loss: 0.000033\n",
      "Epoch 4096, Total Loss: 0.000155, PDE Loss: 0.000102, IC Loss: 0.000052\n",
      "Epoch 4608, Total Loss: 0.000106, PDE Loss: 0.000046, IC Loss: 0.000060\n",
      "Epoch 5120, Total Loss: 0.004000, PDE Loss: 0.001815, IC Loss: 0.002185\n",
      "Epoch 5632, Total Loss: 0.000058, PDE Loss: 0.000019, IC Loss: 0.000039\n",
      "Epoch 6144, Total Loss: 0.000070, PDE Loss: 0.000041, IC Loss: 0.000029\n",
      "Epoch 6656, Total Loss: 0.000027, PDE Loss: 0.000008, IC Loss: 0.000019\n",
      "Epoch 7168, Total Loss: 0.000145, PDE Loss: 0.000106, IC Loss: 0.000039\n",
      "Epoch 7680, Total Loss: 0.000097, PDE Loss: 0.000063, IC Loss: 0.000034\n",
      "Epoch 8192, Total Loss: 0.000019, PDE Loss: 0.000008, IC Loss: 0.000011\n",
      "Epoch 8704, Total Loss: 0.000028, PDE Loss: 0.000014, IC Loss: 0.000014\n",
      "Epoch 9216, Total Loss: 0.000090, PDE Loss: 0.000063, IC Loss: 0.000027\n",
      "Epoch 9728, Total Loss: 0.000034, PDE Loss: 0.000015, IC Loss: 0.000020\n",
      "Training completed in 887.65 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.057858, PDE Loss: 0.000013, IC Loss: 0.057845\n",
      "Epoch 512, Total Loss: 0.001014, PDE Loss: 0.000588, IC Loss: 0.000425\n",
      "Epoch 1024, Total Loss: 0.000150, PDE Loss: 0.000059, IC Loss: 0.000091\n",
      "Epoch 1536, Total Loss: 0.000102, PDE Loss: 0.000055, IC Loss: 0.000048\n",
      "Training completed in 226.37 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.053126, PDE Loss: 0.000040, IC Loss: 0.053086\n",
      "Epoch 512, Total Loss: 0.000651, PDE Loss: 0.000423, IC Loss: 0.000228\n",
      "Epoch 1024, Total Loss: 0.000155, PDE Loss: 0.000091, IC Loss: 0.000064\n",
      "Epoch 1536, Total Loss: 0.000071, PDE Loss: 0.000033, IC Loss: 0.000037\n",
      "Training completed in 445.99 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.057877, PDE Loss: 0.000012, IC Loss: 0.057865\n",
      "Epoch 512, Total Loss: 0.001852, PDE Loss: 0.000744, IC Loss: 0.001108\n",
      "Epoch 1024, Total Loss: 0.000737, PDE Loss: 0.000408, IC Loss: 0.000329\n",
      "Epoch 1536, Total Loss: 0.000093, PDE Loss: 0.000038, IC Loss: 0.000055\n",
      "Epoch 2048, Total Loss: 0.000103, PDE Loss: 0.000051, IC Loss: 0.000052\n",
      "Epoch 2560, Total Loss: 0.000137, PDE Loss: 0.000060, IC Loss: 0.000077\n",
      "Epoch 3072, Total Loss: 0.000075, PDE Loss: 0.000035, IC Loss: 0.000040\n",
      "Epoch 3584, Total Loss: 0.000106, PDE Loss: 0.000052, IC Loss: 0.000053\n",
      "Training completed in 451.16 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.045783, PDE Loss: 0.000029, IC Loss: 0.045754\n",
      "Epoch 512, Total Loss: 0.001368, PDE Loss: 0.000557, IC Loss: 0.000811\n",
      "Epoch 1024, Total Loss: 0.000300, PDE Loss: 0.000143, IC Loss: 0.000158\n",
      "Epoch 1536, Total Loss: 0.000143, PDE Loss: 0.000048, IC Loss: 0.000096\n",
      "Epoch 2048, Total Loss: 0.000083, PDE Loss: 0.000026, IC Loss: 0.000057\n",
      "Epoch 2560, Total Loss: 0.000053, PDE Loss: 0.000013, IC Loss: 0.000040\n",
      "Epoch 3072, Total Loss: 0.000411, PDE Loss: 0.000188, IC Loss: 0.000224\n",
      "Epoch 3584, Total Loss: 0.000162, PDE Loss: 0.000081, IC Loss: 0.000081\n",
      "Training completed in 891.64 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.075113, PDE Loss: 0.000020, IC Loss: 0.075094\n",
      "Epoch 512, Total Loss: 0.001261, PDE Loss: 0.000857, IC Loss: 0.000403\n",
      "Epoch 1024, Total Loss: 0.000377, PDE Loss: 0.000209, IC Loss: 0.000168\n",
      "Epoch 1536, Total Loss: 0.000202, PDE Loss: 0.000063, IC Loss: 0.000139\n",
      "Epoch 2048, Total Loss: 0.000092, PDE Loss: 0.000037, IC Loss: 0.000055\n",
      "Epoch 2560, Total Loss: 0.000065, PDE Loss: 0.000023, IC Loss: 0.000042\n",
      "Epoch 3072, Total Loss: 0.000073, PDE Loss: 0.000031, IC Loss: 0.000042\n",
      "Epoch 3584, Total Loss: 0.000247, PDE Loss: 0.000166, IC Loss: 0.000081\n",
      "Epoch 4096, Total Loss: 0.004168, PDE Loss: 0.002056, IC Loss: 0.002112\n",
      "Epoch 4608, Total Loss: 0.000087, PDE Loss: 0.000053, IC Loss: 0.000034\n",
      "Training completed in 564.20 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.057472, PDE Loss: 0.000016, IC Loss: 0.057457\n",
      "Epoch 512, Total Loss: 0.002575, PDE Loss: 0.001124, IC Loss: 0.001451\n",
      "Epoch 1024, Total Loss: 0.000094, PDE Loss: 0.000038, IC Loss: 0.000056\n",
      "Epoch 1536, Total Loss: 0.000337, PDE Loss: 0.000230, IC Loss: 0.000107\n",
      "Epoch 2048, Total Loss: 0.000068, PDE Loss: 0.000027, IC Loss: 0.000041\n",
      "Epoch 2560, Total Loss: 0.000052, PDE Loss: 0.000020, IC Loss: 0.000032\n",
      "Epoch 3072, Total Loss: 0.000192, PDE Loss: 0.000127, IC Loss: 0.000065\n",
      "Epoch 3584, Total Loss: 0.000047, PDE Loss: 0.000021, IC Loss: 0.000026\n",
      "Epoch 4096, Total Loss: 0.000953, PDE Loss: 0.000524, IC Loss: 0.000430\n",
      "Epoch 4608, Total Loss: 0.000191, PDE Loss: 0.000143, IC Loss: 0.000048\n",
      "Training completed in 1122.73 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.055675, PDE Loss: 0.000020, IC Loss: 0.055655\n",
      "Epoch 512, Total Loss: 0.000977, PDE Loss: 0.000672, IC Loss: 0.000305\n",
      "Epoch 1024, Total Loss: 0.000179, PDE Loss: 0.000121, IC Loss: 0.000058\n",
      "Epoch 1536, Total Loss: 0.000448, PDE Loss: 0.000367, IC Loss: 0.000081\n",
      "Epoch 2048, Total Loss: 0.000067, PDE Loss: 0.000029, IC Loss: 0.000038\n",
      "Epoch 2560, Total Loss: 0.000055, PDE Loss: 0.000025, IC Loss: 0.000030\n",
      "Epoch 3072, Total Loss: 0.000076, PDE Loss: 0.000034, IC Loss: 0.000042\n",
      "Epoch 3584, Total Loss: 0.000082, PDE Loss: 0.000051, IC Loss: 0.000031\n",
      "Epoch 4096, Total Loss: 0.000199, PDE Loss: 0.000100, IC Loss: 0.000099\n",
      "Epoch 4608, Total Loss: 0.000029, PDE Loss: 0.000011, IC Loss: 0.000018\n",
      "Epoch 5120, Total Loss: 0.036419, PDE Loss: 0.014106, IC Loss: 0.022314\n",
      "Epoch 5632, Total Loss: 0.004435, PDE Loss: 0.002256, IC Loss: 0.002179\n",
      "Epoch 6144, Total Loss: 0.000874, PDE Loss: 0.000509, IC Loss: 0.000365\n",
      "Epoch 6656, Total Loss: 0.000291, PDE Loss: 0.000191, IC Loss: 0.000100\n",
      "Epoch 7168, Total Loss: 0.000184, PDE Loss: 0.000123, IC Loss: 0.000061\n",
      "Epoch 7680, Total Loss: 0.000106, PDE Loss: 0.000064, IC Loss: 0.000042\n",
      "Epoch 8192, Total Loss: 0.000093, PDE Loss: 0.000058, IC Loss: 0.000035\n",
      "Epoch 8704, Total Loss: 0.000250, PDE Loss: 0.000220, IC Loss: 0.000031\n",
      "Epoch 9216, Total Loss: 0.000285, PDE Loss: 0.000247, IC Loss: 0.000039\n",
      "Epoch 9728, Total Loss: 0.000307, PDE Loss: 0.000245, IC Loss: 0.000062\n",
      "Training completed in 1134.70 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.052593, PDE Loss: 0.000021, IC Loss: 0.052572\n",
      "Epoch 512, Total Loss: 0.001606, PDE Loss: 0.000909, IC Loss: 0.000697\n",
      "Epoch 1024, Total Loss: 0.000382, PDE Loss: 0.000198, IC Loss: 0.000184\n",
      "Epoch 1536, Total Loss: 0.000340, PDE Loss: 0.000222, IC Loss: 0.000118\n",
      "Epoch 2048, Total Loss: 0.000090, PDE Loss: 0.000031, IC Loss: 0.000060\n",
      "Epoch 2560, Total Loss: 0.000085, PDE Loss: 0.000035, IC Loss: 0.000050\n",
      "Epoch 3072, Total Loss: 0.000094, PDE Loss: 0.000038, IC Loss: 0.000056\n",
      "Epoch 3584, Total Loss: 0.000060, PDE Loss: 0.000021, IC Loss: 0.000039\n",
      "Epoch 4096, Total Loss: 0.000131, PDE Loss: 0.000066, IC Loss: 0.000065\n",
      "Epoch 4608, Total Loss: 0.000061, PDE Loss: 0.000025, IC Loss: 0.000036\n",
      "Epoch 5120, Total Loss: 0.000044, PDE Loss: 0.000016, IC Loss: 0.000029\n",
      "Epoch 5632, Total Loss: 0.000045, PDE Loss: 0.000017, IC Loss: 0.000027\n",
      "Epoch 6144, Total Loss: 0.000087, PDE Loss: 0.000045, IC Loss: 0.000042\n",
      "Epoch 6656, Total Loss: 0.000072, PDE Loss: 0.000044, IC Loss: 0.000027\n",
      "Epoch 7168, Total Loss: 0.000041, PDE Loss: 0.000025, IC Loss: 0.000016\n",
      "Epoch 7680, Total Loss: 0.000055, PDE Loss: 0.000037, IC Loss: 0.000018\n",
      "Epoch 8192, Total Loss: 0.000025, PDE Loss: 0.000017, IC Loss: 0.000009\n",
      "Epoch 8704, Total Loss: 0.000014, PDE Loss: 0.000009, IC Loss: 0.000005\n",
      "Epoch 9216, Total Loss: 0.000023, PDE Loss: 0.000018, IC Loss: 0.000006\n",
      "Epoch 9728, Total Loss: 0.000030, PDE Loss: 0.000016, IC Loss: 0.000015\n",
      "Training completed in 2242.61 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=2048, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.077227, PDE Loss: 0.000003, IC Loss: 0.077224\n",
      "Epoch 512, Total Loss: 0.001403, PDE Loss: 0.001194, IC Loss: 0.000209\n",
      "Epoch 1024, Total Loss: 0.000117, PDE Loss: 0.000056, IC Loss: 0.000061\n",
      "Epoch 1536, Total Loss: 0.000166, PDE Loss: 0.000070, IC Loss: 0.000096\n",
      "Training completed in 293.46 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=2048, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.061836, PDE Loss: 0.000001, IC Loss: 0.061835\n",
      "Epoch 512, Total Loss: 0.001039, PDE Loss: 0.000639, IC Loss: 0.000400\n",
      "Epoch 1024, Total Loss: 0.000107, PDE Loss: 0.000055, IC Loss: 0.000052\n",
      "Epoch 1536, Total Loss: 0.000053, PDE Loss: 0.000020, IC Loss: 0.000034\n",
      "Training completed in 582.60 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=4096, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.077944, PDE Loss: 0.000001, IC Loss: 0.077943\n",
      "Epoch 512, Total Loss: 0.000175, PDE Loss: 0.000081, IC Loss: 0.000094\n",
      "Epoch 1024, Total Loss: 0.000069, PDE Loss: 0.000025, IC Loss: 0.000044\n",
      "Epoch 1536, Total Loss: 0.000164, PDE Loss: 0.000087, IC Loss: 0.000076\n",
      "Epoch 2048, Total Loss: 0.000035, PDE Loss: 0.000019, IC Loss: 0.000015\n",
      "Epoch 2560, Total Loss: 0.000106, PDE Loss: 0.000080, IC Loss: 0.000026\n",
      "Epoch 3072, Total Loss: 0.000056, PDE Loss: 0.000041, IC Loss: 0.000015\n",
      "Epoch 3584, Total Loss: 0.152490, PDE Loss: 0.041564, IC Loss: 0.110926\n",
      "Training completed in 588.00 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=4096, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.037250, PDE Loss: 0.000001, IC Loss: 0.037249\n",
      "Epoch 512, Total Loss: 0.000185, PDE Loss: 0.000076, IC Loss: 0.000109\n",
      "Epoch 1024, Total Loss: 0.000169, PDE Loss: 0.000079, IC Loss: 0.000090\n",
      "Epoch 1536, Total Loss: 0.000179, PDE Loss: 0.000101, IC Loss: 0.000078\n",
      "Epoch 2048, Total Loss: 0.000052, PDE Loss: 0.000025, IC Loss: 0.000026\n",
      "Epoch 2560, Total Loss: 0.000338, PDE Loss: 0.000173, IC Loss: 0.000166\n",
      "Epoch 3072, Total Loss: 0.000020, PDE Loss: 0.000010, IC Loss: 0.000010\n",
      "Epoch 3584, Total Loss: 0.000017, PDE Loss: 0.000009, IC Loss: 0.000008\n",
      "Training completed in 1159.49 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=5120, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.049394, PDE Loss: 0.000007, IC Loss: 0.049387\n",
      "Epoch 512, Total Loss: 0.000476, PDE Loss: 0.000339, IC Loss: 0.000137\n",
      "Epoch 1024, Total Loss: 0.000099, PDE Loss: 0.000042, IC Loss: 0.000058\n",
      "Epoch 1536, Total Loss: 0.000810, PDE Loss: 0.000539, IC Loss: 0.000271\n",
      "Epoch 2048, Total Loss: 0.000045, PDE Loss: 0.000018, IC Loss: 0.000027\n",
      "Epoch 2560, Total Loss: 0.000043, PDE Loss: 0.000018, IC Loss: 0.000025\n",
      "Epoch 3072, Total Loss: 0.000028, PDE Loss: 0.000012, IC Loss: 0.000016\n",
      "Epoch 3584, Total Loss: 0.000089, PDE Loss: 0.000051, IC Loss: 0.000038\n",
      "Epoch 4096, Total Loss: 0.000019, PDE Loss: 0.000008, IC Loss: 0.000011\n",
      "Epoch 4608, Total Loss: 0.039987, PDE Loss: 0.003707, IC Loss: 0.036280\n",
      "Training completed in 730.59 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=5120, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.065197, PDE Loss: 0.000007, IC Loss: 0.065189\n",
      "Epoch 512, Total Loss: 0.001509, PDE Loss: 0.000754, IC Loss: 0.000755\n",
      "Epoch 1024, Total Loss: 0.001010, PDE Loss: 0.000558, IC Loss: 0.000452\n",
      "Epoch 1536, Total Loss: 0.036362, PDE Loss: 0.000652, IC Loss: 0.035709\n",
      "Epoch 2048, Total Loss: 0.034885, PDE Loss: 0.000395, IC Loss: 0.034490\n",
      "Epoch 2560, Total Loss: 0.034286, PDE Loss: 0.000506, IC Loss: 0.033779\n",
      "Epoch 3072, Total Loss: 0.033246, PDE Loss: 0.000811, IC Loss: 0.032435\n",
      "Epoch 3584, Total Loss: 0.030269, PDE Loss: 0.001588, IC Loss: 0.028680\n",
      "Epoch 4096, Total Loss: 0.021665, PDE Loss: 0.002605, IC Loss: 0.019060\n",
      "Epoch 4608, Total Loss: 0.016366, PDE Loss: 0.004643, IC Loss: 0.011724\n",
      "Training completed in 1456.80 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=10240, collocation_points=1024\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.053433, PDE Loss: 0.000004, IC Loss: 0.053429\n",
      "Epoch 512, Total Loss: 0.000202, PDE Loss: 0.000092, IC Loss: 0.000110\n",
      "Epoch 1024, Total Loss: 0.000038, PDE Loss: 0.000017, IC Loss: 0.000021\n",
      "Epoch 1536, Total Loss: 0.000391, PDE Loss: 0.000210, IC Loss: 0.000181\n",
      "Epoch 2048, Total Loss: 0.000180, PDE Loss: 0.000140, IC Loss: 0.000040\n",
      "Epoch 2560, Total Loss: 0.000122, PDE Loss: 0.000085, IC Loss: 0.000037\n",
      "Epoch 3072, Total Loss: 0.000013, PDE Loss: 0.000008, IC Loss: 0.000005\n",
      "Epoch 3584, Total Loss: 1.064030, PDE Loss: 0.476192, IC Loss: 0.587839\n",
      "Epoch 4096, Total Loss: 0.112288, PDE Loss: 0.045079, IC Loss: 0.067209\n",
      "Epoch 4608, Total Loss: 0.074413, PDE Loss: 0.015950, IC Loss: 0.058462\n",
      "Epoch 5120, Total Loss: 0.059608, PDE Loss: 0.008153, IC Loss: 0.051455\n",
      "Epoch 5632, Total Loss: 0.050768, PDE Loss: 0.004790, IC Loss: 0.045978\n",
      "Epoch 6144, Total Loss: 0.044995, PDE Loss: 0.003039, IC Loss: 0.041956\n",
      "Epoch 6656, Total Loss: 0.041099, PDE Loss: 0.002035, IC Loss: 0.039064\n",
      "Epoch 7168, Total Loss: 0.038255, PDE Loss: 0.001443, IC Loss: 0.036812\n",
      "Epoch 7680, Total Loss: 0.035839, PDE Loss: 0.001095, IC Loss: 0.034744\n",
      "Epoch 8192, Total Loss: 0.033429, PDE Loss: 0.000882, IC Loss: 0.032547\n",
      "Epoch 8704, Total Loss: 0.030794, PDE Loss: 0.000757, IC Loss: 0.030038\n",
      "Epoch 9216, Total Loss: 0.046955, PDE Loss: 0.018889, IC Loss: 0.028065\n",
      "Epoch 9728, Total Loss: 0.024904, PDE Loss: 0.000623, IC Loss: 0.024281\n",
      "Training completed in 1469.25 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=10240, collocation_points=2048\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.065424, PDE Loss: 0.000003, IC Loss: 0.065421\n",
      "Epoch 512, Total Loss: 0.000380, PDE Loss: 0.000186, IC Loss: 0.000195\n",
      "Epoch 1024, Total Loss: 0.000084, PDE Loss: 0.000046, IC Loss: 0.000037\n",
      "Epoch 1536, Total Loss: 0.000061, PDE Loss: 0.000031, IC Loss: 0.000030\n",
      "Epoch 2048, Total Loss: 0.000038, PDE Loss: 0.000014, IC Loss: 0.000024\n",
      "Epoch 2560, Total Loss: 0.000066, PDE Loss: 0.000032, IC Loss: 0.000034\n",
      "Epoch 3072, Total Loss: 0.000030, PDE Loss: 0.000013, IC Loss: 0.000018\n",
      "Epoch 3584, Total Loss: 0.000023, PDE Loss: 0.000009, IC Loss: 0.000014\n",
      "Epoch 4096, Total Loss: 0.102043, PDE Loss: 0.023000, IC Loss: 0.079043\n",
      "Epoch 4608, Total Loss: 0.034873, PDE Loss: 0.004580, IC Loss: 0.030293\n",
      "Epoch 5120, Total Loss: 0.029582, PDE Loss: 0.002716, IC Loss: 0.026866\n",
      "Epoch 5632, Total Loss: 0.026652, PDE Loss: 0.002567, IC Loss: 0.024085\n",
      "Epoch 6144, Total Loss: 0.023569, PDE Loss: 0.002824, IC Loss: 0.020745\n",
      "Epoch 6656, Total Loss: 0.020062, PDE Loss: 0.003298, IC Loss: 0.016764\n",
      "Epoch 7168, Total Loss: 0.016017, PDE Loss: 0.003679, IC Loss: 0.012339\n",
      "Epoch 7680, Total Loss: 0.011225, PDE Loss: 0.003605, IC Loss: 0.007620\n",
      "Epoch 8192, Total Loss: 0.006544, PDE Loss: 0.002922, IC Loss: 0.003621\n",
      "Epoch 8704, Total Loss: 0.005639, PDE Loss: 0.002853, IC Loss: 0.002785\n",
      "Epoch 9216, Total Loss: 0.002285, PDE Loss: 0.001207, IC Loss: 0.001078\n",
      "Epoch 9728, Total Loss: 0.001098, PDE Loss: 0.000602, IC Loss: 0.000496\n",
      "Training completed in 2904.29 seconds.\n"
     ]
    }
   ],
   "source": [
    "INITIAL = 'polynomial'\n",
    "def run_experiments(activation_funcs, hidden_layers_neurons_list, epochs_list, collocation_points_list):\n",
    "    u_ex = np.loadtxt(\"u_ex.txt\").T  # (512, 205)\n",
    "    \n",
    "    results = []\n",
    "    for (act_name, act_func), (hidden_layers, neurons), epochs, cp in itertools.product(\n",
    "            activation_funcs, hidden_layers_neurons_list, epochs_list, collocation_points_list):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"\\nRunning experiment: activation={act_name}, hidden_layers={hidden_layers}, neurons={neurons}, epochs={epochs}, collocation_points={cp}\")\n",
    "        \n",
    "        theta_colloc_exp = torch.rand(cp, 1, device=device) * xmax\n",
    "        t_colloc_exp = torch.rand(cp, 1, device=device) * Tf\n",
    "        \n",
    "        model = PINN(activation=act_func, hidden_layers=hidden_layers, neurons=neurons).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        u0 = initialise(theta_init)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        loss_history = _trainer(u0, model, optimizer, theta_colloc_exp, t_colloc_exp, num_epochs=epochs)\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            u_n = model(x_test).reshape(u_ex.shape).cpu().numpy()\n",
    "        \n",
    "        deviation = np.mean((u_n - u_ex)**2)\n",
    "        \n",
    "        results.append({\n",
    "            'activation': act_name,\n",
    "            'hidden_layers, neurons': (hidden_layers, neurons),\n",
    "            'epochs': epochs,\n",
    "            'collocation_points': cp,\n",
    "            'deviation': deviation,\n",
    "            'training_time': training_time,\n",
    "\n",
    "        }); \n",
    "    \n",
    "    fastest = sorted(results, key=lambda x: x[\"weighted_avg\"])\n",
    "    best = sorted(results, key=lambda x: x[\"deviation\"])\n",
    "    output_data = {\n",
    "        \"Fastest\": {\n",
    "            \"description\": \"Results sorted by Training time\",\n",
    "            \"results\": fastest\n",
    "        },\n",
    "    \n",
    "        \"Best\": {\n",
    "            \"description\": \"Results sorted by deviation\",\n",
    "            \"results\": best\n",
    "        }\n",
    "    }\n",
    "    output_file = \"experiment_results.json\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "    return fastest, best\n",
    "\n",
    "\n",
    "# -------------------------------------------- #\n",
    "# Parameter grid \n",
    "activation_funcs = [\n",
    "    (\"tanh\", torch.tanh),\n",
    "    (\"sin\", lambda x: torch.sin(x))\n",
    "]\n",
    "hidden_layers_neurons_list = [\n",
    "    (4, 64),\n",
    "    (4, 128),\n",
    "    (6, 128),\n",
    "    (6, 256),\n",
    "    (8, 256),\n",
    "]\n",
    "epochs_list = [2048, 4096, 5120, 10240]\n",
    "collocation_points_list = [1024, 2048]\n",
    "\n",
    "# fastest, best = run_experiments(activation_funcs, hidden_layers_neurons_list, epochs_list, collocation_points_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_key(exp):\n",
    "    return (exp[\"activation\"], \n",
    "            tuple(exp[\"hidden_layers, neurons\"]), \n",
    "            exp[\"epochs\"], \n",
    "            exp[\"collocation_points\"])\n",
    "\n",
    "with open(\"experiment_results.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fastest = data[\"Fastest\"][\"results\"]\n",
    "best = data[\"Best\"][\"results\"]\n",
    "\n",
    "experiments = {}\n",
    "for exp in fastest + best:\n",
    "    key = get_unique_key(exp)\n",
    "    experiments[key] = exp\n",
    "    \n",
    "exp_list = list(experiments.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minmax normalized composite score for balancing minimal and optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 experiments based on composite score:\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 0.0001228807900057513, 'training_time': 34.676011085510254, 'composite_score': 0.0006587120619260076}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 7.861544949624488e-05, 'training_time': 37.933730125427246, 'composite_score': 0.0014319737939505663}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 128], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 7.747055319846629e-05, 'training_time': 58.79557514190674, 'composite_score': 0.008692520718273275}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 5.2962631952024245e-05, 'training_time': 67.24257802963257, 'composite_score': 0.011435705736845141}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 128], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 8.322645868894856e-05, 'training_time': 66.92218494415283, 'composite_score': 0.011571540918388822}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 2048, 'collocation_points': 2048, 'deviation': 0.00042852840386981564, 'training_time': 62.24307107925415, 'composite_score': 0.012764709156885122}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 2048, 'collocation_points': 2048, 'deviation': 9.396003029929244e-05, 'training_time': 71.28997254371643, 'composite_score': 0.013181396845257523}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 4.3711407677040294e-05, 'training_time': 75.92699980735779, 'composite_score': 0.01438638930034626}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [6, 128], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 9.710682539382176e-05, 'training_time': 81.49561047554016, 'composite_score': 0.01676357789257744}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 5120, 'collocation_points': 1024, 'deviation': 4.410067502737426e-05, 'training_time': 86.39026832580566, 'composite_score': 0.018035799180499617}\n"
     ]
    }
   ],
   "source": [
    "training_times = [exp[\"training_time\"] for exp in exp_list]\n",
    "deviations = [exp[\"deviation\"] for exp in exp_list]\n",
    "min_time, max_time = min(training_times), max(training_times)\n",
    "min_dev, max_dev = min(deviations), max(deviations)\n",
    "\n",
    "def normalize(x, min_val, max_val):\n",
    "    return (x - min_val) / (max_val - min_val) if max_val != min_val else 0\n",
    "\n",
    "for exp in exp_list:\n",
    "    norm_time = normalize(exp[\"training_time\"], min_time, max_time)\n",
    "    norm_dev = normalize(exp[\"deviation\"], min_dev, max_dev)\n",
    "    exp[\"composite_score\"] = norm_time + norm_dev\n",
    "\n",
    "top_10 = sorted(exp_list, key=lambda x: x[\"composite_score\"])[:10]\n",
    "print(\"Top 10 experiments based on composite score:\")\n",
    "for exp in top_10:\n",
    "    print(exp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank aggregation for optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 experiments based on rank aggregation:\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 4.3711407677040294e-05, 'training_time': 75.92699980735779, 'time_rank': 8, 'dev_rank': 3, 'aggregate_rank': 11}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 5120, 'collocation_points': 1024, 'deviation': 4.410067502737426e-05, 'training_time': 86.39026832580566, 'time_rank': 10, 'dev_rank': 4, 'aggregate_rank': 14}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 128], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 4.232997382630036e-05, 'training_time': 113.4130265712738, 'time_rank': 13, 'dev_rank': 1, 'aggregate_rank': 14}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 10240, 'collocation_points': 1024, 'deviation': 4.234831232206919e-05, 'training_time': 171.66479301452637, 'time_rank': 24, 'dev_rank': 2, 'aggregate_rank': 26}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 5120, 'collocation_points': 1024, 'deviation': 4.956191759570236e-05, 'training_time': 94.99149703979492, 'time_rank': 12, 'dev_rank': 16, 'aggregate_rank': 28}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 4096, 'collocation_points': 2048, 'deviation': 4.6648168922599885e-05, 'training_time': 142.4206371307373, 'time_rank': 19, 'dev_rank': 9, 'aggregate_rank': 28}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 5.2962631952024245e-05, 'training_time': 67.24257802963257, 'time_rank': 6, 'dev_rank': 23, 'aggregate_rank': 29}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [6, 128], 'epochs': 5120, 'collocation_points': 1024, 'deviation': 4.672187943987987e-05, 'training_time': 203.80518507957458, 'time_rank': 29, 'dev_rank': 10, 'aggregate_rank': 39}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 128], 'epochs': 5120, 'collocation_points': 1024, 'deviation': 5.317374798533358e-05, 'training_time': 141.77655339241028, 'time_rank': 18, 'dev_rank': 24, 'aggregate_rank': 42}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 10240, 'collocation_points': 2048, 'deviation': 4.4615233910230715e-05, 'training_time': 320.1263132095337, 'time_rank': 41, 'dev_rank': 5, 'aggregate_rank': 46}\n"
     ]
    }
   ],
   "source": [
    "sorted_by_time = sorted(exp_list, key=lambda x: x[\"training_time\"])\n",
    "for rank, exp in enumerate(sorted_by_time, start=1):\n",
    "    exp[\"time_rank\"] = rank\n",
    "\n",
    "sorted_by_dev = sorted(exp_list, key=lambda x: x[\"deviation\"])\n",
    "for rank, exp in enumerate(sorted_by_dev, start=1):\n",
    "    exp[\"dev_rank\"] = rank\n",
    "\n",
    "for exp in exp_list:\n",
    "    exp[\"aggregate_rank\"] = exp[\"time_rank\"] + exp[\"dev_rank\"]\n",
    "\n",
    "top_10 = sorted(exp_list, key=lambda x: x[\"aggregate_rank\"])[:10]\n",
    "print(\"Top 10 experiments based on rank aggregation:\")\n",
    "for exp in top_10:\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pareto front domination for minimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pareto front experiments:\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 0.0001228807900057513, 'training_time': 34.676011085510254}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 7.861544949624488e-05, 'training_time': 37.933730125427246}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 128], 'epochs': 2048, 'collocation_points': 1024, 'deviation': 7.747055319846629e-05, 'training_time': 58.79557514190674}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 64], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 5.2962631952024245e-05, 'training_time': 67.24257802963257}\n",
      "{'activation': 'sin', 'hidden_layers, neurons': [4, 64], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 4.3711407677040294e-05, 'training_time': 75.92699980735779}\n",
      "{'activation': 'tanh', 'hidden_layers, neurons': [4, 128], 'epochs': 4096, 'collocation_points': 1024, 'deviation': 4.232997382630036e-05, 'training_time': 113.4130265712738}\n"
     ]
    }
   ],
   "source": [
    "def is_dominated(exp, others):\n",
    "    for other in others:\n",
    "        if (other[\"training_time\"] <= exp[\"training_time\"] and\n",
    "            other[\"deviation\"] <= exp[\"deviation\"] and\n",
    "            (other[\"training_time\"] < exp[\"training_time\"] or other[\"deviation\"] < exp[\"deviation\"])):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "pareto_front = [exp for exp in exp_list if not is_dominated(exp, exp_list)]\n",
    "print(\"Pareto front experiments:\")\n",
    "for exp in pareto_front:\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential match for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: Experiments from top 20 Best found in top 20 Fastest:\n",
      "Best index: 0, Fastest index: 12, Experiment key: ('tanh', (4, 128), 4096, 1024)\n",
      "Best index: 2, Fastest index: 7, Experiment key: ('sin', (4, 64), 4096, 1024)\n",
      "Best index: 3, Fastest index: 9, Experiment key: ('tanh', (4, 64), 5120, 1024)\n",
      "Best index: 8, Fastest index: 18, Experiment key: ('sin', (4, 64), 4096, 2048)\n",
      "Best index: 15, Fastest index: 11, Experiment key: ('sin', (4, 64), 5120, 1024)\n",
      "\n",
      "Matches: Experiments from top 20 Fastest found in top 20 Best:\n",
      "Fastest index: 7, Best index: 2, Experiment key: ('sin', (4, 64), 4096, 1024)\n",
      "Fastest index: 9, Best index: 3, Experiment key: ('tanh', (4, 64), 5120, 1024)\n",
      "Fastest index: 11, Best index: 15, Experiment key: ('sin', (4, 64), 5120, 1024)\n",
      "Fastest index: 12, Best index: 0, Experiment key: ('tanh', (4, 128), 4096, 1024)\n",
      "Fastest index: 18, Best index: 8, Experiment key: ('sin', (4, 64), 4096, 2048)\n"
     ]
    }
   ],
   "source": [
    "fastest_top20 = data[\"Fastest\"][\"results\"][:20]\n",
    "best_top20 = data[\"Best\"][\"results\"][:20]\n",
    "\n",
    "fastest_keys = [get_unique_key(exp) for exp in fastest_top20]\n",
    "best_keys = [get_unique_key(exp) for exp in best_top20]\n",
    "\n",
    "matches_from_best = []\n",
    "for idx_best, key in enumerate(best_keys):\n",
    "    if key in fastest_keys:\n",
    "        idx_fastest = fastest_keys.index(key)\n",
    "        matches_from_best.append((idx_best, idx_fastest, key))\n",
    "\n",
    "print(\"Matches: Experiments from top 20 Best found in top 20 Fastest:\")\n",
    "for idx_best, idx_fastest, key in matches_from_best:\n",
    "    print(f\"Best index: {idx_best}, Fastest index: {idx_fastest}, Experiment key: {key}\")\n",
    "\n",
    "matches_from_fastest = []\n",
    "for idx_fastest, key in enumerate(fastest_keys):\n",
    "    if key in best_keys:\n",
    "        idx_best = best_keys.index(key)\n",
    "        matches_from_fastest.append((idx_fastest, idx_best, key))\n",
    "\n",
    "print(\"\\nMatches: Experiments from top 20 Fastest found in top 20 Best:\")\n",
    "for idx_fastest, idx_best, key in matches_from_fastest:\n",
    "    print(f\"Fastest index: {idx_fastest}, Best index: {idx_best}, Experiment key: {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'activation': <built-in method tanh of type object at 0x00007FF9B11B8560>, 'hln': (4, 64), 'epochs': 5120, 'collocation_points': 1024}\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.067108, PDE Loss: 0.000507, IC Loss: 0.066601\n",
      "Epoch 512, Total Loss: 0.004229, PDE Loss: 0.001586, IC Loss: 0.002643\n",
      "Epoch 1024, Total Loss: 0.000294, PDE Loss: 0.000175, IC Loss: 0.000118\n",
      "Epoch 1536, Total Loss: 0.000139, PDE Loss: 0.000082, IC Loss: 0.000056\n",
      "Epoch 2048, Total Loss: 0.000217, PDE Loss: 0.000156, IC Loss: 0.000062\n",
      "Epoch 2560, Total Loss: 0.000098, PDE Loss: 0.000064, IC Loss: 0.000034\n",
      "Epoch 3072, Total Loss: 0.000058, PDE Loss: 0.000028, IC Loss: 0.000030\n",
      "Epoch 3584, Total Loss: 0.000376, PDE Loss: 0.000297, IC Loss: 0.000079\n",
      "Epoch 4096, Total Loss: 0.000129, PDE Loss: 0.000079, IC Loss: 0.000050\n",
      "Epoch 4608, Total Loss: 0.000040, PDE Loss: 0.000020, IC Loss: 0.000020\n",
      "Training completed in 86.28 seconds.\n",
      "\n",
      "\n",
      "{'activation': <function <lambda> at 0x0000026BFBCFAD40>, 'hln': (4, 64), 'epochs': 4096, 'collocation_points': 1024}\n",
      "Polynomial initial condition: u0(x) = (6/pi^3) * ((3pi/2 - x) * (x - pi/2)) for x in [pi/2, 3pi/2]; 0 otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.095219, PDE Loss: 0.000363, IC Loss: 0.094856\n",
      "Epoch 512, Total Loss: 0.002661, PDE Loss: 0.001340, IC Loss: 0.001321\n",
      "Epoch 1024, Total Loss: 0.001021, PDE Loss: 0.000549, IC Loss: 0.000472\n",
      "Epoch 1536, Total Loss: 0.000503, PDE Loss: 0.000210, IC Loss: 0.000292\n",
      "Epoch 2048, Total Loss: 0.000207, PDE Loss: 0.000102, IC Loss: 0.000106\n",
      "Epoch 2560, Total Loss: 0.000115, PDE Loss: 0.000043, IC Loss: 0.000072\n",
      "Epoch 3072, Total Loss: 0.000082, PDE Loss: 0.000032, IC Loss: 0.000050\n",
      "Epoch 3584, Total Loss: 0.000067, PDE Loss: 0.000025, IC Loss: 0.000042\n",
      "Training completed in 78.36 seconds.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparisons\n",
    "INITIAL = 'polynomial'\n",
    "architectures = [\n",
    "    {\"activation\": torch.tanh, \"hln\": (4, 64),   \"epochs\": 5120,  \"collocation_points\": 1024},\n",
    "    {\"activation\": lambda x: torch.sin(x), \"hln\": (4, 64),  \"epochs\": 4096, \"collocation_points\": 1024}\n",
    "]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "u_ex = np.loadtxt(\"u_ex.txt\").T  # shape: (205, 512)\n",
    "\n",
    "os.makedirs(\"comparison/plots\", exist_ok=True)\n",
    "os.makedirs(\"comparison/loss\", exist_ok=True)\n",
    "\n",
    "predictions = []\n",
    "for i, arch in enumerate(architectures):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"\\n\\n{arch}\")\n",
    "    \n",
    "    theta_colloc_exp = torch.rand(arch[\"collocation_points\"], 1, device=device) * xmax\n",
    "    t_colloc_exp = torch.rand(arch[\"collocation_points\"], 1, device=device) * Tf\n",
    "    \n",
    "    model = PINN(activation=arch[\"activation\"], \n",
    "                 hidden_layers=arch[\"hln\"][0], \n",
    "                 neurons=arch[\"hln\"][1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    u0 = initialise(theta_init)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    loss_history = _trainer(u0, model, optimizer, theta_colloc_exp, t_colloc_exp, num_epochs=arch[\"epochs\"])\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "    \n",
    "    # Loss history\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(loss_history, 'b-')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss History for Architecture {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"comparison/loss/loss_history_arch_{i+1}.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(x_test).reshape(u_ex.shape).cpu().numpy()\n",
    "    predictions.append(u_pred)\n",
    "\n",
    "# Comparison plot at t = Tf\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_line, u_ex[:, -1], 'k-', label=\"Exact Solution\")\n",
    "for i, u_pred in enumerate(predictions):\n",
    "    plt.plot(x_line, u_pred[:, -1], '--', label=f\"Arch {i+1}\")\n",
    "plt.xlabel(r'$\\theta$')\n",
    "plt.ylabel('u')\n",
    "plt.title(f\"Comparison at t = {Tf}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison/plots/comparison_Tf.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Problem Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: activation=relu, hidden_layers=6, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.007356, PDE Loss: 0.000001, IC Loss: 0.007356\n",
      "Epoch 512, Total Loss: 0.000056, PDE Loss: 0.000026, IC Loss: 0.000031\n",
      "Epoch 1024, Total Loss: 0.000082, PDE Loss: 0.000024, IC Loss: 0.000058\n",
      "Epoch 1536, Total Loss: 0.000046, PDE Loss: 0.000023, IC Loss: 0.000023\n",
      "Epoch 2048, Total Loss: 0.000059, PDE Loss: 0.000026, IC Loss: 0.000033\n",
      "Epoch 2560, Total Loss: 0.000052, PDE Loss: 0.000032, IC Loss: 0.000020\n",
      "Epoch 3072, Total Loss: 0.000095, PDE Loss: 0.000040, IC Loss: 0.000055\n",
      "Epoch 3584, Total Loss: 0.000054, PDE Loss: 0.000029, IC Loss: 0.000025\n",
      "Epoch 4096, Total Loss: 0.000062, PDE Loss: 0.000039, IC Loss: 0.000024\n",
      "Epoch 4608, Total Loss: 0.000063, PDE Loss: 0.000036, IC Loss: 0.000027\n",
      "Training completed in 384.71 seconds.\n",
      "\n",
      "Running experiment: activation=relu, hidden_layers=6, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.033612, PDE Loss: 0.000001, IC Loss: 0.033611\n",
      "Epoch 512, Total Loss: 0.000112, PDE Loss: 0.000050, IC Loss: 0.000062\n",
      "Epoch 1024, Total Loss: 0.000083, PDE Loss: 0.000038, IC Loss: 0.000045\n",
      "Epoch 1536, Total Loss: 0.000059, PDE Loss: 0.000027, IC Loss: 0.000032\n",
      "Epoch 2048, Total Loss: 0.000064, PDE Loss: 0.000038, IC Loss: 0.000027\n",
      "Epoch 2560, Total Loss: 0.000081, PDE Loss: 0.000037, IC Loss: 0.000045\n",
      "Epoch 3072, Total Loss: 0.000099, PDE Loss: 0.000037, IC Loss: 0.000062\n",
      "Epoch 3584, Total Loss: 0.000092, PDE Loss: 0.000056, IC Loss: 0.000036\n",
      "Epoch 4096, Total Loss: 0.000105, PDE Loss: 0.000075, IC Loss: 0.000031\n",
      "Epoch 4608, Total Loss: 0.000097, PDE Loss: 0.000068, IC Loss: 0.000029\n",
      "Epoch 5120, Total Loss: 0.000067, PDE Loss: 0.000042, IC Loss: 0.000025\n",
      "Epoch 5632, Total Loss: 0.000151, PDE Loss: 0.000065, IC Loss: 0.000087\n",
      "Epoch 6144, Total Loss: 0.000073, PDE Loss: 0.000039, IC Loss: 0.000034\n",
      "Epoch 6656, Total Loss: 0.000118, PDE Loss: 0.000050, IC Loss: 0.000068\n",
      "Epoch 7168, Total Loss: 0.000090, PDE Loss: 0.000058, IC Loss: 0.000032\n",
      "Epoch 7680, Total Loss: 0.000128, PDE Loss: 0.000089, IC Loss: 0.000039\n",
      "Epoch 8192, Total Loss: 0.000113, PDE Loss: 0.000082, IC Loss: 0.000031\n",
      "Epoch 8704, Total Loss: 0.000120, PDE Loss: 0.000089, IC Loss: 0.000031\n",
      "Epoch 9216, Total Loss: 0.000145, PDE Loss: 0.000094, IC Loss: 0.000051\n",
      "Epoch 9728, Total Loss: 0.000086, PDE Loss: 0.000058, IC Loss: 0.000028\n",
      "Training completed in 775.23 seconds.\n",
      "\n",
      "Running experiment: activation=relu, hidden_layers=6, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.034391, PDE Loss: 0.000000, IC Loss: 0.034391\n",
      "Epoch 512, Total Loss: 0.000390, PDE Loss: 0.000061, IC Loss: 0.000328\n",
      "Epoch 1024, Total Loss: 0.000069, PDE Loss: 0.000036, IC Loss: 0.000033\n",
      "Epoch 1536, Total Loss: 0.000063, PDE Loss: 0.000030, IC Loss: 0.000033\n",
      "Epoch 2048, Total Loss: 0.000073, PDE Loss: 0.000040, IC Loss: 0.000033\n",
      "Epoch 2560, Total Loss: 0.000046, PDE Loss: 0.000019, IC Loss: 0.000026\n",
      "Epoch 3072, Total Loss: 0.000030, PDE Loss: 0.000011, IC Loss: 0.000020\n",
      "Epoch 3584, Total Loss: 0.000054, PDE Loss: 0.000032, IC Loss: 0.000022\n",
      "Epoch 4096, Total Loss: 0.000028, PDE Loss: 0.000016, IC Loss: 0.000012\n",
      "Epoch 4608, Total Loss: 0.000042, PDE Loss: 0.000014, IC Loss: 0.000028\n",
      "Training completed in 984.64 seconds.\n",
      "\n",
      "Running experiment: activation=relu, hidden_layers=6, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.041149, PDE Loss: 0.000001, IC Loss: 0.041149\n",
      "Epoch 512, Total Loss: 0.000087, PDE Loss: 0.000039, IC Loss: 0.000048\n",
      "Epoch 1024, Total Loss: 0.000040, PDE Loss: 0.000012, IC Loss: 0.000028\n",
      "Epoch 1536, Total Loss: 0.000037, PDE Loss: 0.000013, IC Loss: 0.000024\n",
      "Epoch 2048, Total Loss: 0.000038, PDE Loss: 0.000018, IC Loss: 0.000021\n",
      "Epoch 2560, Total Loss: 0.000053, PDE Loss: 0.000017, IC Loss: 0.000035\n",
      "Epoch 3072, Total Loss: 0.000035, PDE Loss: 0.000014, IC Loss: 0.000021\n",
      "Epoch 3584, Total Loss: 0.000030, PDE Loss: 0.000014, IC Loss: 0.000016\n",
      "Epoch 4096, Total Loss: 0.000041, PDE Loss: 0.000023, IC Loss: 0.000018\n",
      "Epoch 4608, Total Loss: 0.000052, PDE Loss: 0.000024, IC Loss: 0.000028\n",
      "Epoch 5120, Total Loss: 0.000036, PDE Loss: 0.000015, IC Loss: 0.000021\n",
      "Epoch 5632, Total Loss: 0.000101, PDE Loss: 0.000048, IC Loss: 0.000053\n",
      "Epoch 6144, Total Loss: 0.000044, PDE Loss: 0.000027, IC Loss: 0.000017\n",
      "Epoch 6656, Total Loss: 0.000048, PDE Loss: 0.000023, IC Loss: 0.000025\n",
      "Epoch 7168, Total Loss: 0.000146, PDE Loss: 0.000057, IC Loss: 0.000088\n",
      "Epoch 7680, Total Loss: 0.000061, PDE Loss: 0.000033, IC Loss: 0.000028\n",
      "Epoch 8192, Total Loss: 0.000413, PDE Loss: 0.000308, IC Loss: 0.000105\n",
      "Epoch 8704, Total Loss: 0.000409, PDE Loss: 0.000328, IC Loss: 0.000080\n",
      "Epoch 9216, Total Loss: 0.000419, PDE Loss: 0.000288, IC Loss: 0.000132\n",
      "Epoch 9728, Total Loss: 0.000424, PDE Loss: 0.000323, IC Loss: 0.000101\n",
      "Training completed in 1956.60 seconds.\n",
      "\n",
      "Running experiment: activation=relu, hidden_layers=8, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.023775, PDE Loss: 0.000000, IC Loss: 0.023775\n",
      "Epoch 512, Total Loss: 0.000084, PDE Loss: 0.000037, IC Loss: 0.000046\n",
      "Epoch 1024, Total Loss: 0.000095, PDE Loss: 0.000034, IC Loss: 0.000061\n",
      "Epoch 1536, Total Loss: 0.000078, PDE Loss: 0.000041, IC Loss: 0.000037\n",
      "Epoch 2048, Total Loss: 0.000057, PDE Loss: 0.000022, IC Loss: 0.000035\n",
      "Epoch 2560, Total Loss: 0.000089, PDE Loss: 0.000056, IC Loss: 0.000033\n",
      "Epoch 3072, Total Loss: 0.000124, PDE Loss: 0.000061, IC Loss: 0.000063\n",
      "Epoch 3584, Total Loss: 0.000078, PDE Loss: 0.000050, IC Loss: 0.000028\n",
      "Epoch 4096, Total Loss: 0.000086, PDE Loss: 0.000057, IC Loss: 0.000029\n",
      "Epoch 4608, Total Loss: 0.000111, PDE Loss: 0.000066, IC Loss: 0.000045\n",
      "Training completed in 495.22 seconds.\n",
      "\n",
      "Running experiment: activation=relu, hidden_layers=8, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.027307, PDE Loss: 0.000000, IC Loss: 0.027307\n",
      "Epoch 512, Total Loss: 0.000100, PDE Loss: 0.000048, IC Loss: 0.000052\n",
      "Epoch 1024, Total Loss: 0.000061, PDE Loss: 0.000025, IC Loss: 0.000036\n",
      "Epoch 1536, Total Loss: 0.000059, PDE Loss: 0.000034, IC Loss: 0.000025\n",
      "Epoch 2048, Total Loss: 0.000058, PDE Loss: 0.000033, IC Loss: 0.000025\n",
      "Epoch 2560, Total Loss: 0.000069, PDE Loss: 0.000032, IC Loss: 0.000037\n",
      "Epoch 3072, Total Loss: 0.000053, PDE Loss: 0.000027, IC Loss: 0.000026\n",
      "Epoch 3584, Total Loss: 0.000060, PDE Loss: 0.000033, IC Loss: 0.000027\n",
      "Epoch 4096, Total Loss: 0.000065, PDE Loss: 0.000045, IC Loss: 0.000020\n",
      "Epoch 4608, Total Loss: 0.000094, PDE Loss: 0.000057, IC Loss: 0.000037\n",
      "Epoch 5120, Total Loss: 0.000080, PDE Loss: 0.000046, IC Loss: 0.000034\n",
      "Epoch 5632, Total Loss: 0.000078, PDE Loss: 0.000053, IC Loss: 0.000025\n",
      "Epoch 6144, Total Loss: 0.000099, PDE Loss: 0.000067, IC Loss: 0.000032\n",
      "Epoch 6656, Total Loss: 0.000094, PDE Loss: 0.000066, IC Loss: 0.000028\n",
      "Epoch 7168, Total Loss: 0.000078, PDE Loss: 0.000052, IC Loss: 0.000026\n",
      "Epoch 7680, Total Loss: 0.000107, PDE Loss: 0.000066, IC Loss: 0.000040\n",
      "Epoch 8192, Total Loss: 0.000103, PDE Loss: 0.000072, IC Loss: 0.000031\n",
      "Epoch 8704, Total Loss: 0.000072, PDE Loss: 0.000041, IC Loss: 0.000032\n",
      "Epoch 9216, Total Loss: 0.000073, PDE Loss: 0.000045, IC Loss: 0.000028\n",
      "Epoch 9728, Total Loss: 0.000454, PDE Loss: 0.000276, IC Loss: 0.000179\n",
      "Training completed in 990.03 seconds.\n",
      "\n",
      "Running experiment: activation=relu, hidden_layers=8, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.014620, PDE Loss: 0.000000, IC Loss: 0.014620\n",
      "Epoch 512, Total Loss: 0.000073, PDE Loss: 0.000022, IC Loss: 0.000050\n",
      "Epoch 1024, Total Loss: 0.000056, PDE Loss: 0.000019, IC Loss: 0.000036\n",
      "Epoch 1536, Total Loss: 0.000050, PDE Loss: 0.000021, IC Loss: 0.000029\n",
      "Epoch 2048, Total Loss: 0.000031, PDE Loss: 0.000011, IC Loss: 0.000019\n",
      "Epoch 2560, Total Loss: 0.000062, PDE Loss: 0.000041, IC Loss: 0.000022\n",
      "Epoch 3072, Total Loss: 0.000098, PDE Loss: 0.000046, IC Loss: 0.000052\n",
      "Epoch 3584, Total Loss: 0.000047, PDE Loss: 0.000023, IC Loss: 0.000024\n",
      "Epoch 4096, Total Loss: 0.000047, PDE Loss: 0.000017, IC Loss: 0.000030\n",
      "Epoch 4608, Total Loss: 0.000098, PDE Loss: 0.000063, IC Loss: 0.000035\n",
      "Training completed in 1274.20 seconds.\n",
      "\n",
      "Running experiment: activation=relu, hidden_layers=8, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.057819, PDE Loss: 0.000000, IC Loss: 0.057819\n",
      "Epoch 512, Total Loss: 0.000096, PDE Loss: 0.000021, IC Loss: 0.000075\n",
      "Epoch 1024, Total Loss: 0.000125, PDE Loss: 0.000028, IC Loss: 0.000097\n",
      "Epoch 1536, Total Loss: 0.000064, PDE Loss: 0.000031, IC Loss: 0.000034\n",
      "Epoch 2048, Total Loss: 0.000166, PDE Loss: 0.000136, IC Loss: 0.000030\n",
      "Epoch 2560, Total Loss: 0.000029, PDE Loss: 0.000012, IC Loss: 0.000017\n",
      "Epoch 3072, Total Loss: 0.000044, PDE Loss: 0.000024, IC Loss: 0.000019\n",
      "Epoch 3584, Total Loss: 0.000046, PDE Loss: 0.000025, IC Loss: 0.000021\n",
      "Epoch 4096, Total Loss: 0.000031, PDE Loss: 0.000017, IC Loss: 0.000014\n",
      "Epoch 4608, Total Loss: 0.000032, PDE Loss: 0.000015, IC Loss: 0.000018\n",
      "Epoch 5120, Total Loss: 0.000036, PDE Loss: 0.000017, IC Loss: 0.000019\n",
      "Epoch 5632, Total Loss: 0.000029, PDE Loss: 0.000014, IC Loss: 0.000015\n",
      "Epoch 6144, Total Loss: 0.000029, PDE Loss: 0.000013, IC Loss: 0.000015\n",
      "Epoch 6656, Total Loss: 0.000057, PDE Loss: 0.000034, IC Loss: 0.000022\n",
      "Epoch 7168, Total Loss: 0.000039, PDE Loss: 0.000025, IC Loss: 0.000014\n",
      "Epoch 7680, Total Loss: 0.000073, PDE Loss: 0.000045, IC Loss: 0.000028\n",
      "Epoch 8192, Total Loss: 0.000054, PDE Loss: 0.000030, IC Loss: 0.000024\n",
      "Epoch 8704, Total Loss: 0.000021, PDE Loss: 0.000012, IC Loss: 0.000009\n",
      "Epoch 9216, Total Loss: 0.000101, PDE Loss: 0.000075, IC Loss: 0.000026\n",
      "Epoch 9728, Total Loss: 0.000131, PDE Loss: 0.000103, IC Loss: 0.000028\n",
      "Training completed in 2557.80 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=6, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.056527, PDE Loss: 0.000001, IC Loss: 0.056527\n",
      "Epoch 512, Total Loss: 0.000114, PDE Loss: 0.000061, IC Loss: 0.000053\n",
      "Epoch 1024, Total Loss: 0.000095, PDE Loss: 0.000053, IC Loss: 0.000042\n",
      "Epoch 1536, Total Loss: 0.000117, PDE Loss: 0.000065, IC Loss: 0.000052\n",
      "Epoch 2048, Total Loss: 0.000108, PDE Loss: 0.000062, IC Loss: 0.000046\n",
      "Epoch 2560, Total Loss: 0.000128, PDE Loss: 0.000048, IC Loss: 0.000080\n",
      "Epoch 3072, Total Loss: 0.000098, PDE Loss: 0.000063, IC Loss: 0.000035\n",
      "Epoch 3584, Total Loss: 0.000134, PDE Loss: 0.000063, IC Loss: 0.000071\n",
      "Epoch 4096, Total Loss: 0.000115, PDE Loss: 0.000085, IC Loss: 0.000030\n",
      "Epoch 4608, Total Loss: 0.000114, PDE Loss: 0.000075, IC Loss: 0.000040\n",
      "Training completed in 392.95 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=6, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.020588, PDE Loss: 0.000000, IC Loss: 0.020587\n",
      "Epoch 512, Total Loss: 0.000122, PDE Loss: 0.000055, IC Loss: 0.000067\n",
      "Epoch 1024, Total Loss: 0.000075, PDE Loss: 0.000032, IC Loss: 0.000043\n",
      "Epoch 1536, Total Loss: 0.000056, PDE Loss: 0.000033, IC Loss: 0.000023\n",
      "Epoch 2048, Total Loss: 0.000128, PDE Loss: 0.000070, IC Loss: 0.000059\n",
      "Epoch 2560, Total Loss: 0.000059, PDE Loss: 0.000037, IC Loss: 0.000022\n",
      "Epoch 3072, Total Loss: 0.000066, PDE Loss: 0.000037, IC Loss: 0.000029\n",
      "Epoch 3584, Total Loss: 0.000059, PDE Loss: 0.000036, IC Loss: 0.000023\n",
      "Epoch 4096, Total Loss: 0.000105, PDE Loss: 0.000055, IC Loss: 0.000050\n",
      "Epoch 4608, Total Loss: 0.000056, PDE Loss: 0.000023, IC Loss: 0.000033\n",
      "Epoch 5120, Total Loss: 0.000084, PDE Loss: 0.000056, IC Loss: 0.000028\n",
      "Epoch 5632, Total Loss: 0.000060, PDE Loss: 0.000035, IC Loss: 0.000025\n",
      "Epoch 6144, Total Loss: 0.000056, PDE Loss: 0.000031, IC Loss: 0.000025\n",
      "Epoch 6656, Total Loss: 0.000069, PDE Loss: 0.000046, IC Loss: 0.000023\n",
      "Epoch 7168, Total Loss: 0.000070, PDE Loss: 0.000043, IC Loss: 0.000027\n",
      "Epoch 7680, Total Loss: 0.000068, PDE Loss: 0.000043, IC Loss: 0.000025\n",
      "Epoch 8192, Total Loss: 0.000062, PDE Loss: 0.000041, IC Loss: 0.000021\n",
      "Epoch 8704, Total Loss: 0.000083, PDE Loss: 0.000061, IC Loss: 0.000022\n",
      "Epoch 9216, Total Loss: 0.000089, PDE Loss: 0.000066, IC Loss: 0.000023\n",
      "Epoch 9728, Total Loss: 0.000099, PDE Loss: 0.000068, IC Loss: 0.000032\n",
      "Training completed in 784.40 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=6, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.020000, PDE Loss: 0.000000, IC Loss: 0.020000\n",
      "Epoch 512, Total Loss: 0.000056, PDE Loss: 0.000018, IC Loss: 0.000038\n",
      "Epoch 1024, Total Loss: 0.000073, PDE Loss: 0.000037, IC Loss: 0.000035\n",
      "Epoch 1536, Total Loss: 0.000038, PDE Loss: 0.000013, IC Loss: 0.000024\n",
      "Epoch 2048, Total Loss: 0.000098, PDE Loss: 0.000045, IC Loss: 0.000053\n",
      "Epoch 2560, Total Loss: 0.000073, PDE Loss: 0.000012, IC Loss: 0.000061\n",
      "Epoch 3072, Total Loss: 0.000044, PDE Loss: 0.000021, IC Loss: 0.000024\n",
      "Epoch 3584, Total Loss: 0.000034, PDE Loss: 0.000019, IC Loss: 0.000015\n",
      "Epoch 4096, Total Loss: 0.000184, PDE Loss: 0.000073, IC Loss: 0.000111\n",
      "Epoch 4608, Total Loss: 0.000069, PDE Loss: 0.000030, IC Loss: 0.000039\n",
      "Training completed in 1026.65 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=6, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.022999, PDE Loss: 0.000000, IC Loss: 0.022999\n",
      "Epoch 512, Total Loss: 0.000055, PDE Loss: 0.000013, IC Loss: 0.000042\n",
      "Epoch 1024, Total Loss: 0.000060, PDE Loss: 0.000021, IC Loss: 0.000039\n",
      "Epoch 1536, Total Loss: 0.000059, PDE Loss: 0.000016, IC Loss: 0.000043\n",
      "Epoch 2048, Total Loss: 0.000038, PDE Loss: 0.000011, IC Loss: 0.000027\n",
      "Epoch 2560, Total Loss: 0.000072, PDE Loss: 0.000034, IC Loss: 0.000038\n",
      "Epoch 3072, Total Loss: 0.000044, PDE Loss: 0.000018, IC Loss: 0.000026\n",
      "Epoch 3584, Total Loss: 0.000061, PDE Loss: 0.000033, IC Loss: 0.000028\n",
      "Epoch 4096, Total Loss: 0.000052, PDE Loss: 0.000017, IC Loss: 0.000035\n",
      "Epoch 4608, Total Loss: 0.000049, PDE Loss: 0.000024, IC Loss: 0.000025\n",
      "Epoch 5120, Total Loss: 0.000051, PDE Loss: 0.000019, IC Loss: 0.000033\n",
      "Epoch 5632, Total Loss: 0.000037, PDE Loss: 0.000018, IC Loss: 0.000020\n",
      "Epoch 6144, Total Loss: 0.000199, PDE Loss: 0.000018, IC Loss: 0.000181\n",
      "Epoch 6656, Total Loss: 0.000039, PDE Loss: 0.000016, IC Loss: 0.000023\n",
      "Epoch 7168, Total Loss: 0.000045, PDE Loss: 0.000016, IC Loss: 0.000029\n",
      "Epoch 7680, Total Loss: 0.000031, PDE Loss: 0.000016, IC Loss: 0.000014\n",
      "Epoch 8192, Total Loss: 0.000033, PDE Loss: 0.000015, IC Loss: 0.000018\n",
      "Epoch 8704, Total Loss: 0.000034, PDE Loss: 0.000019, IC Loss: 0.000015\n",
      "Epoch 9216, Total Loss: 0.000053, PDE Loss: 0.000025, IC Loss: 0.000028\n",
      "Epoch 9728, Total Loss: 0.000042, PDE Loss: 0.000028, IC Loss: 0.000014\n",
      "Training completed in 2040.32 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=8, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.062223, PDE Loss: 0.000000, IC Loss: 0.062222\n",
      "Epoch 512, Total Loss: 0.000072, PDE Loss: 0.000026, IC Loss: 0.000046\n",
      "Epoch 1024, Total Loss: 0.000051, PDE Loss: 0.000022, IC Loss: 0.000029\n",
      "Epoch 1536, Total Loss: 0.000066, PDE Loss: 0.000032, IC Loss: 0.000034\n",
      "Epoch 2048, Total Loss: 0.000103, PDE Loss: 0.000046, IC Loss: 0.000057\n",
      "Epoch 2560, Total Loss: 0.000060, PDE Loss: 0.000027, IC Loss: 0.000033\n",
      "Epoch 3072, Total Loss: 0.000046, PDE Loss: 0.000027, IC Loss: 0.000019\n",
      "Epoch 3584, Total Loss: 0.000031, PDE Loss: 0.000017, IC Loss: 0.000014\n",
      "Epoch 4096, Total Loss: 0.000070, PDE Loss: 0.000026, IC Loss: 0.000045\n",
      "Epoch 4608, Total Loss: 0.000043, PDE Loss: 0.000024, IC Loss: 0.000019\n",
      "Training completed in 505.04 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=8, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.022892, PDE Loss: 0.000000, IC Loss: 0.022892\n",
      "Epoch 512, Total Loss: 0.000110, PDE Loss: 0.000046, IC Loss: 0.000064\n",
      "Epoch 1024, Total Loss: 0.000047, PDE Loss: 0.000018, IC Loss: 0.000029\n",
      "Epoch 1536, Total Loss: 0.000056, PDE Loss: 0.000025, IC Loss: 0.000030\n",
      "Epoch 2048, Total Loss: 0.000143, PDE Loss: 0.000062, IC Loss: 0.000081\n",
      "Epoch 2560, Total Loss: 0.000052, PDE Loss: 0.000024, IC Loss: 0.000028\n",
      "Epoch 3072, Total Loss: 0.000110, PDE Loss: 0.000033, IC Loss: 0.000077\n",
      "Epoch 3584, Total Loss: 0.000034, PDE Loss: 0.000017, IC Loss: 0.000017\n",
      "Epoch 4096, Total Loss: 0.000043, PDE Loss: 0.000022, IC Loss: 0.000020\n",
      "Epoch 4608, Total Loss: 0.000063, PDE Loss: 0.000018, IC Loss: 0.000045\n",
      "Epoch 5120, Total Loss: 0.000060, PDE Loss: 0.000021, IC Loss: 0.000039\n",
      "Epoch 5632, Total Loss: 0.000043, PDE Loss: 0.000023, IC Loss: 0.000020\n",
      "Epoch 6144, Total Loss: 0.000043, PDE Loss: 0.000024, IC Loss: 0.000019\n",
      "Epoch 6656, Total Loss: 0.000036, PDE Loss: 0.000017, IC Loss: 0.000019\n",
      "Epoch 7168, Total Loss: 0.000044, PDE Loss: 0.000018, IC Loss: 0.000025\n",
      "Epoch 7680, Total Loss: 0.000038, PDE Loss: 0.000024, IC Loss: 0.000014\n",
      "Epoch 8192, Total Loss: 0.000052, PDE Loss: 0.000033, IC Loss: 0.000020\n",
      "Epoch 8704, Total Loss: 0.000140, PDE Loss: 0.000038, IC Loss: 0.000103\n",
      "Epoch 9216, Total Loss: 0.000046, PDE Loss: 0.000027, IC Loss: 0.000019\n",
      "Epoch 9728, Total Loss: 0.000780, PDE Loss: 0.000434, IC Loss: 0.000346\n",
      "Training completed in 1009.30 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=8, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.020628, PDE Loss: 0.000000, IC Loss: 0.020628\n",
      "Epoch 512, Total Loss: 0.000067, PDE Loss: 0.000015, IC Loss: 0.000051\n",
      "Epoch 1024, Total Loss: 0.000195, PDE Loss: 0.000069, IC Loss: 0.000126\n",
      "Epoch 1536, Total Loss: 0.000047, PDE Loss: 0.000021, IC Loss: 0.000027\n",
      "Epoch 2048, Total Loss: 0.000140, PDE Loss: 0.000049, IC Loss: 0.000091\n",
      "Epoch 2560, Total Loss: 0.000028, PDE Loss: 0.000011, IC Loss: 0.000017\n",
      "Epoch 3072, Total Loss: 0.000027, PDE Loss: 0.000010, IC Loss: 0.000016\n",
      "Epoch 3584, Total Loss: 0.000036, PDE Loss: 0.000015, IC Loss: 0.000020\n",
      "Epoch 4096, Total Loss: 0.000028, PDE Loss: 0.000011, IC Loss: 0.000017\n",
      "Epoch 4608, Total Loss: 0.000033, PDE Loss: 0.000012, IC Loss: 0.000021\n",
      "Training completed in 1336.43 seconds.\n",
      "\n",
      "Running experiment: activation=leaky_relu, hidden_layers=8, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.020356, PDE Loss: 0.000000, IC Loss: 0.020356\n",
      "Epoch 512, Total Loss: 0.000089, PDE Loss: 0.000037, IC Loss: 0.000052\n",
      "Epoch 1024, Total Loss: 0.000052, PDE Loss: 0.000015, IC Loss: 0.000037\n",
      "Epoch 1536, Total Loss: 0.000053, PDE Loss: 0.000017, IC Loss: 0.000035\n",
      "Epoch 2048, Total Loss: 0.000038, PDE Loss: 0.000015, IC Loss: 0.000023\n",
      "Epoch 2560, Total Loss: 0.000042, PDE Loss: 0.000020, IC Loss: 0.000022\n",
      "Epoch 3072, Total Loss: 0.000027, PDE Loss: 0.000011, IC Loss: 0.000016\n",
      "Epoch 3584, Total Loss: 0.000036, PDE Loss: 0.000011, IC Loss: 0.000025\n",
      "Epoch 4096, Total Loss: 0.000047, PDE Loss: 0.000016, IC Loss: 0.000030\n",
      "Epoch 4608, Total Loss: 0.000071, PDE Loss: 0.000027, IC Loss: 0.000044\n",
      "Epoch 5120, Total Loss: 0.000042, PDE Loss: 0.000015, IC Loss: 0.000027\n",
      "Epoch 5632, Total Loss: 0.000027, PDE Loss: 0.000011, IC Loss: 0.000016\n",
      "Epoch 6144, Total Loss: 0.000028, PDE Loss: 0.000013, IC Loss: 0.000016\n",
      "Epoch 6656, Total Loss: 0.000060, PDE Loss: 0.000029, IC Loss: 0.000031\n",
      "Epoch 7168, Total Loss: 0.000038, PDE Loss: 0.000020, IC Loss: 0.000018\n",
      "Epoch 7680, Total Loss: 0.000092, PDE Loss: 0.000025, IC Loss: 0.000066\n",
      "Epoch 8192, Total Loss: 0.000047, PDE Loss: 0.000033, IC Loss: 0.000015\n",
      "Epoch 8704, Total Loss: 0.000026, PDE Loss: 0.000011, IC Loss: 0.000014\n",
      "Epoch 9216, Total Loss: 0.000040, PDE Loss: 0.000023, IC Loss: 0.000017\n",
      "Epoch 9728, Total Loss: 0.000082, PDE Loss: 0.000045, IC Loss: 0.000037\n",
      "Training completed in 2672.41 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.028589, PDE Loss: 0.000017, IC Loss: 0.028572\n",
      "Epoch 512, Total Loss: 0.000421, PDE Loss: 0.000036, IC Loss: 0.000386\n",
      "Epoch 1024, Total Loss: 0.000297, PDE Loss: 0.000047, IC Loss: 0.000251\n",
      "Epoch 1536, Total Loss: 0.000146, PDE Loss: 0.000008, IC Loss: 0.000137\n",
      "Epoch 2048, Total Loss: 0.000094, PDE Loss: 0.000005, IC Loss: 0.000089\n",
      "Epoch 2560, Total Loss: 0.000075, PDE Loss: 0.000004, IC Loss: 0.000071\n",
      "Epoch 3072, Total Loss: 0.000062, PDE Loss: 0.000003, IC Loss: 0.000058\n",
      "Epoch 3584, Total Loss: 0.000051, PDE Loss: 0.000003, IC Loss: 0.000048\n",
      "Epoch 4096, Total Loss: 0.000043, PDE Loss: 0.000002, IC Loss: 0.000040\n",
      "Epoch 4608, Total Loss: 0.000029, PDE Loss: 0.000002, IC Loss: 0.000027\n",
      "Training completed in 448.20 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.006131, PDE Loss: 0.000010, IC Loss: 0.006121\n",
      "Epoch 512, Total Loss: 0.000344, PDE Loss: 0.000017, IC Loss: 0.000327\n",
      "Epoch 1024, Total Loss: 0.000277, PDE Loss: 0.000022, IC Loss: 0.000255\n",
      "Epoch 1536, Total Loss: 0.000311, PDE Loss: 0.000030, IC Loss: 0.000281\n",
      "Epoch 2048, Total Loss: 0.000150, PDE Loss: 0.000008, IC Loss: 0.000141\n",
      "Epoch 2560, Total Loss: 0.000074, PDE Loss: 0.000007, IC Loss: 0.000067\n",
      "Epoch 3072, Total Loss: 0.000046, PDE Loss: 0.000003, IC Loss: 0.000042\n",
      "Epoch 3584, Total Loss: 0.000027, PDE Loss: 0.000003, IC Loss: 0.000024\n",
      "Epoch 4096, Total Loss: 0.000025, PDE Loss: 0.000005, IC Loss: 0.000020\n",
      "Epoch 4608, Total Loss: 0.000013, PDE Loss: 0.000002, IC Loss: 0.000011\n",
      "Epoch 5120, Total Loss: 0.000017, PDE Loss: 0.000004, IC Loss: 0.000013\n",
      "Epoch 5632, Total Loss: 0.000009, PDE Loss: 0.000001, IC Loss: 0.000008\n",
      "Epoch 6144, Total Loss: 0.000005, PDE Loss: 0.000001, IC Loss: 0.000004\n",
      "Epoch 6656, Total Loss: 0.000004, PDE Loss: 0.000001, IC Loss: 0.000003\n",
      "Epoch 7168, Total Loss: 0.000017, PDE Loss: 0.000010, IC Loss: 0.000007\n",
      "Epoch 7680, Total Loss: 0.000002, PDE Loss: 0.000001, IC Loss: 0.000001\n",
      "Epoch 8192, Total Loss: 0.000002, PDE Loss: 0.000002, IC Loss: 0.000001\n",
      "Epoch 8704, Total Loss: 0.000027, PDE Loss: 0.000003, IC Loss: 0.000024\n",
      "Epoch 9216, Total Loss: 0.000015, PDE Loss: 0.000002, IC Loss: 0.000014\n",
      "Epoch 9728, Total Loss: 0.000055, PDE Loss: 0.000018, IC Loss: 0.000038\n",
      "Training completed in 896.28 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.009894, PDE Loss: 0.000082, IC Loss: 0.009812\n",
      "Epoch 512, Total Loss: 0.000284, PDE Loss: 0.000024, IC Loss: 0.000260\n",
      "Epoch 1024, Total Loss: 0.000233, PDE Loss: 0.000010, IC Loss: 0.000223\n",
      "Epoch 1536, Total Loss: 0.000236, PDE Loss: 0.000033, IC Loss: 0.000203\n",
      "Epoch 2048, Total Loss: 0.000073, PDE Loss: 0.000009, IC Loss: 0.000064\n",
      "Epoch 2560, Total Loss: 0.000047, PDE Loss: 0.000006, IC Loss: 0.000041\n",
      "Epoch 3072, Total Loss: 0.000040, PDE Loss: 0.000004, IC Loss: 0.000036\n",
      "Epoch 3584, Total Loss: 0.000029, PDE Loss: 0.000004, IC Loss: 0.000025\n",
      "Epoch 4096, Total Loss: 0.000023, PDE Loss: 0.000003, IC Loss: 0.000020\n",
      "Epoch 4608, Total Loss: 0.000016, PDE Loss: 0.000002, IC Loss: 0.000013\n",
      "Training completed in 1133.82 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=6, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.016227, PDE Loss: 0.000013, IC Loss: 0.016213\n",
      "Epoch 512, Total Loss: 0.000285, PDE Loss: 0.000027, IC Loss: 0.000257\n",
      "Epoch 1024, Total Loss: 0.000227, PDE Loss: 0.000011, IC Loss: 0.000216\n",
      "Epoch 1536, Total Loss: 0.000095, PDE Loss: 0.000011, IC Loss: 0.000084\n",
      "Epoch 2048, Total Loss: 0.000051, PDE Loss: 0.000006, IC Loss: 0.000045\n",
      "Epoch 2560, Total Loss: 0.000037, PDE Loss: 0.000005, IC Loss: 0.000033\n",
      "Epoch 3072, Total Loss: 0.000034, PDE Loss: 0.000004, IC Loss: 0.000030\n",
      "Epoch 3584, Total Loss: 0.000026, PDE Loss: 0.000003, IC Loss: 0.000024\n",
      "Epoch 4096, Total Loss: 0.000018, PDE Loss: 0.000002, IC Loss: 0.000015\n",
      "Epoch 4608, Total Loss: 0.000015, PDE Loss: 0.000002, IC Loss: 0.000013\n",
      "Epoch 5120, Total Loss: 0.000012, PDE Loss: 0.000001, IC Loss: 0.000011\n",
      "Epoch 5632, Total Loss: 0.000017, PDE Loss: 0.000006, IC Loss: 0.000010\n",
      "Epoch 6144, Total Loss: 0.000008, PDE Loss: 0.000001, IC Loss: 0.000007\n",
      "Epoch 6656, Total Loss: 0.000134, PDE Loss: 0.000032, IC Loss: 0.000102\n",
      "Epoch 7168, Total Loss: 0.000074, PDE Loss: 0.000009, IC Loss: 0.000065\n",
      "Epoch 7680, Total Loss: 0.000051, PDE Loss: 0.000005, IC Loss: 0.000045\n",
      "Epoch 8192, Total Loss: 0.000036, PDE Loss: 0.000004, IC Loss: 0.000032\n",
      "Epoch 8704, Total Loss: 0.000029, PDE Loss: 0.000003, IC Loss: 0.000026\n",
      "Epoch 9216, Total Loss: 0.000024, PDE Loss: 0.000003, IC Loss: 0.000021\n",
      "Epoch 9728, Total Loss: 0.000021, PDE Loss: 0.000003, IC Loss: 0.000018\n",
      "Training completed in 2262.41 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.025185, PDE Loss: 0.000008, IC Loss: 0.025177\n",
      "Epoch 512, Total Loss: 0.000109, PDE Loss: 0.000010, IC Loss: 0.000098\n",
      "Epoch 1024, Total Loss: 0.000044, PDE Loss: 0.000007, IC Loss: 0.000037\n",
      "Epoch 1536, Total Loss: 0.000025, PDE Loss: 0.000004, IC Loss: 0.000020\n",
      "Epoch 2048, Total Loss: 0.000019, PDE Loss: 0.000005, IC Loss: 0.000014\n",
      "Epoch 2560, Total Loss: 0.000013, PDE Loss: 0.000003, IC Loss: 0.000010\n",
      "Epoch 3072, Total Loss: 0.000015, PDE Loss: 0.000002, IC Loss: 0.000013\n",
      "Epoch 3584, Total Loss: 0.000006, PDE Loss: 0.000001, IC Loss: 0.000005\n",
      "Epoch 4096, Total Loss: 0.000016, PDE Loss: 0.000002, IC Loss: 0.000014\n",
      "Epoch 4608, Total Loss: 0.000017, PDE Loss: 0.000003, IC Loss: 0.000013\n",
      "Training completed in 575.49 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.085956, PDE Loss: 0.000010, IC Loss: 0.085945\n",
      "Epoch 512, Total Loss: 0.000268, PDE Loss: 0.000054, IC Loss: 0.000213\n",
      "Epoch 1024, Total Loss: 0.000045, PDE Loss: 0.000005, IC Loss: 0.000040\n",
      "Epoch 1536, Total Loss: 0.000021, PDE Loss: 0.000003, IC Loss: 0.000018\n",
      "Epoch 2048, Total Loss: 0.000015, PDE Loss: 0.000002, IC Loss: 0.000013\n",
      "Epoch 2560, Total Loss: 0.000014, PDE Loss: 0.000003, IC Loss: 0.000012\n",
      "Epoch 3072, Total Loss: 0.000039, PDE Loss: 0.000018, IC Loss: 0.000020\n",
      "Epoch 3584, Total Loss: 0.000013, PDE Loss: 0.000007, IC Loss: 0.000006\n",
      "Epoch 4096, Total Loss: 0.000478, PDE Loss: 0.000206, IC Loss: 0.000272\n",
      "Epoch 4608, Total Loss: 0.000093, PDE Loss: 0.000021, IC Loss: 0.000072\n",
      "Epoch 5120, Total Loss: 0.000047, PDE Loss: 0.000006, IC Loss: 0.000041\n",
      "Epoch 5632, Total Loss: 0.000033, PDE Loss: 0.000004, IC Loss: 0.000029\n",
      "Epoch 6144, Total Loss: 0.000026, PDE Loss: 0.000004, IC Loss: 0.000022\n",
      "Epoch 6656, Total Loss: 0.000019, PDE Loss: 0.000002, IC Loss: 0.000017\n",
      "Epoch 7168, Total Loss: 0.000014, PDE Loss: 0.000002, IC Loss: 0.000012\n",
      "Epoch 7680, Total Loss: 0.000009, PDE Loss: 0.000001, IC Loss: 0.000008\n",
      "Epoch 8192, Total Loss: 0.000005, PDE Loss: 0.000001, IC Loss: 0.000005\n",
      "Epoch 8704, Total Loss: 0.000004, PDE Loss: 0.000001, IC Loss: 0.000003\n",
      "Epoch 9216, Total Loss: 0.000012, PDE Loss: 0.000002, IC Loss: 0.000010\n",
      "Epoch 9728, Total Loss: 0.000002, PDE Loss: 0.000001, IC Loss: 0.000001\n",
      "Training completed in 1151.18 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.028811, PDE Loss: 0.000008, IC Loss: 0.028803\n",
      "Epoch 512, Total Loss: 0.000097, PDE Loss: 0.000010, IC Loss: 0.000087\n",
      "Epoch 1024, Total Loss: 0.000039, PDE Loss: 0.000003, IC Loss: 0.000036\n",
      "Epoch 1536, Total Loss: 0.000023, PDE Loss: 0.000002, IC Loss: 0.000021\n",
      "Epoch 2048, Total Loss: 0.000737, PDE Loss: 0.000312, IC Loss: 0.000426\n",
      "Epoch 2560, Total Loss: 0.000101, PDE Loss: 0.000019, IC Loss: 0.000082\n",
      "Epoch 3072, Total Loss: 0.000065, PDE Loss: 0.000012, IC Loss: 0.000053\n",
      "Epoch 3584, Total Loss: 0.000046, PDE Loss: 0.000008, IC Loss: 0.000038\n",
      "Epoch 4096, Total Loss: 0.000040, PDE Loss: 0.000006, IC Loss: 0.000034\n",
      "Epoch 4608, Total Loss: 0.000032, PDE Loss: 0.000004, IC Loss: 0.000028\n",
      "Training completed in 1473.19 seconds.\n",
      "\n",
      "Running experiment: activation=sin, hidden_layers=8, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.034627, PDE Loss: 0.000005, IC Loss: 0.034622\n",
      "Epoch 512, Total Loss: 0.000151, PDE Loss: 0.000019, IC Loss: 0.000132\n",
      "Epoch 1024, Total Loss: 0.000028, PDE Loss: 0.000004, IC Loss: 0.000024\n",
      "Epoch 1536, Total Loss: 0.000017, PDE Loss: 0.000002, IC Loss: 0.000015\n",
      "Epoch 2048, Total Loss: 0.000018, PDE Loss: 0.000006, IC Loss: 0.000012\n",
      "Epoch 2560, Total Loss: 0.330073, PDE Loss: 0.302446, IC Loss: 0.027628\n",
      "Epoch 3072, Total Loss: 0.003514, PDE Loss: 0.001605, IC Loss: 0.001908\n",
      "Epoch 3584, Total Loss: 0.002263, PDE Loss: 0.000847, IC Loss: 0.001416\n",
      "Epoch 4096, Total Loss: 0.001768, PDE Loss: 0.000601, IC Loss: 0.001167\n",
      "Epoch 4608, Total Loss: 0.001477, PDE Loss: 0.000478, IC Loss: 0.000999\n",
      "Epoch 5120, Total Loss: 0.001277, PDE Loss: 0.000408, IC Loss: 0.000868\n",
      "Epoch 5632, Total Loss: 0.001105, PDE Loss: 0.000361, IC Loss: 0.000744\n",
      "Epoch 6144, Total Loss: 0.000940, PDE Loss: 0.000323, IC Loss: 0.000616\n",
      "Epoch 6656, Total Loss: 0.000773, PDE Loss: 0.000289, IC Loss: 0.000484\n",
      "Epoch 7168, Total Loss: 0.000610, PDE Loss: 0.000250, IC Loss: 0.000360\n",
      "Epoch 7680, Total Loss: 0.000460, PDE Loss: 0.000208, IC Loss: 0.000252\n",
      "Epoch 8192, Total Loss: 0.000333, PDE Loss: 0.000163, IC Loss: 0.000170\n",
      "Epoch 8704, Total Loss: 0.000238, PDE Loss: 0.000119, IC Loss: 0.000119\n",
      "Epoch 9216, Total Loss: 0.000230, PDE Loss: 0.000137, IC Loss: 0.000093\n",
      "Epoch 9728, Total Loss: 0.000151, PDE Loss: 0.000076, IC Loss: 0.000075\n",
      "Training completed in 2955.89 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=6, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.080950, PDE Loss: 0.000000, IC Loss: 0.080949\n",
      "Epoch 512, Total Loss: 0.000439, PDE Loss: 0.000019, IC Loss: 0.000421\n",
      "Epoch 1024, Total Loss: 0.000295, PDE Loss: 0.000020, IC Loss: 0.000275\n",
      "Epoch 1536, Total Loss: 0.000184, PDE Loss: 0.000006, IC Loss: 0.000177\n",
      "Epoch 2048, Total Loss: 0.000126, PDE Loss: 0.000010, IC Loss: 0.000117\n",
      "Epoch 2560, Total Loss: 0.000081, PDE Loss: 0.000011, IC Loss: 0.000070\n",
      "Epoch 3072, Total Loss: 0.000085, PDE Loss: 0.000007, IC Loss: 0.000079\n",
      "Epoch 3584, Total Loss: 0.000044, PDE Loss: 0.000009, IC Loss: 0.000035\n",
      "Epoch 4096, Total Loss: 0.000049, PDE Loss: 0.000008, IC Loss: 0.000040\n",
      "Epoch 4608, Total Loss: 0.000015, PDE Loss: 0.000001, IC Loss: 0.000014\n",
      "Training completed in 404.84 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=6, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.513169, PDE Loss: 0.001140, IC Loss: 0.512029\n",
      "Epoch 512, Total Loss: 0.000603, PDE Loss: 0.000104, IC Loss: 0.000499\n",
      "Epoch 1024, Total Loss: 0.000314, PDE Loss: 0.000019, IC Loss: 0.000295\n",
      "Epoch 1536, Total Loss: 0.000249, PDE Loss: 0.000029, IC Loss: 0.000220\n",
      "Epoch 2048, Total Loss: 0.000166, PDE Loss: 0.000009, IC Loss: 0.000156\n",
      "Epoch 2560, Total Loss: 0.000101, PDE Loss: 0.000009, IC Loss: 0.000093\n",
      "Epoch 3072, Total Loss: 0.000102, PDE Loss: 0.000012, IC Loss: 0.000089\n",
      "Epoch 3584, Total Loss: 0.000037, PDE Loss: 0.000003, IC Loss: 0.000034\n",
      "Epoch 4096, Total Loss: 0.000027, PDE Loss: 0.000002, IC Loss: 0.000025\n",
      "Epoch 4608, Total Loss: 0.000030, PDE Loss: 0.000003, IC Loss: 0.000027\n",
      "Epoch 5120, Total Loss: 0.000015, PDE Loss: 0.000001, IC Loss: 0.000014\n",
      "Epoch 5632, Total Loss: 0.000016, PDE Loss: 0.000002, IC Loss: 0.000015\n",
      "Epoch 6144, Total Loss: 0.000010, PDE Loss: 0.000001, IC Loss: 0.000009\n",
      "Epoch 6656, Total Loss: 0.000027, PDE Loss: 0.000002, IC Loss: 0.000025\n",
      "Epoch 7168, Total Loss: 0.000015, PDE Loss: 0.000001, IC Loss: 0.000014\n",
      "Epoch 7680, Total Loss: 0.000026, PDE Loss: 0.000003, IC Loss: 0.000023\n",
      "Epoch 8192, Total Loss: 0.000016, PDE Loss: 0.000001, IC Loss: 0.000015\n",
      "Epoch 8704, Total Loss: 0.000005, PDE Loss: 0.000000, IC Loss: 0.000005\n",
      "Epoch 9216, Total Loss: 0.000019, PDE Loss: 0.000001, IC Loss: 0.000018\n",
      "Epoch 9728, Total Loss: 0.000011, PDE Loss: 0.000001, IC Loss: 0.000010\n",
      "Training completed in 810.14 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=6, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 1.053714, PDE Loss: 0.001087, IC Loss: 1.052627\n",
      "Epoch 512, Total Loss: 0.000490, PDE Loss: 0.000070, IC Loss: 0.000420\n",
      "Epoch 1024, Total Loss: 0.000222, PDE Loss: 0.000017, IC Loss: 0.000205\n",
      "Epoch 1536, Total Loss: 0.000168, PDE Loss: 0.000007, IC Loss: 0.000160\n",
      "Epoch 2048, Total Loss: 0.000151, PDE Loss: 0.000007, IC Loss: 0.000144\n",
      "Epoch 2560, Total Loss: 0.000120, PDE Loss: 0.000007, IC Loss: 0.000113\n",
      "Epoch 3072, Total Loss: 0.000097, PDE Loss: 0.000007, IC Loss: 0.000090\n",
      "Epoch 3584, Total Loss: 0.000304, PDE Loss: 0.000010, IC Loss: 0.000294\n",
      "Epoch 4096, Total Loss: 0.000063, PDE Loss: 0.000004, IC Loss: 0.000059\n",
      "Epoch 4608, Total Loss: 0.000124, PDE Loss: 0.000008, IC Loss: 0.000116\n",
      "Training completed in 1057.87 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=6, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.002813, PDE Loss: 0.000001, IC Loss: 0.002812\n",
      "Epoch 512, Total Loss: 0.000765, PDE Loss: 0.000147, IC Loss: 0.000617\n",
      "Epoch 1024, Total Loss: 0.000187, PDE Loss: 0.000016, IC Loss: 0.000171\n",
      "Epoch 1536, Total Loss: 0.000128, PDE Loss: 0.000012, IC Loss: 0.000116\n",
      "Epoch 2048, Total Loss: 0.000079, PDE Loss: 0.000009, IC Loss: 0.000070\n",
      "Epoch 2560, Total Loss: 0.000056, PDE Loss: 0.000007, IC Loss: 0.000050\n",
      "Epoch 3072, Total Loss: 0.000053, PDE Loss: 0.000016, IC Loss: 0.000036\n",
      "Epoch 3584, Total Loss: 0.000084, PDE Loss: 0.000013, IC Loss: 0.000071\n",
      "Epoch 4096, Total Loss: 0.000032, PDE Loss: 0.000009, IC Loss: 0.000024\n",
      "Epoch 4608, Total Loss: 0.000126, PDE Loss: 0.000010, IC Loss: 0.000116\n",
      "Epoch 5120, Total Loss: 0.000115, PDE Loss: 0.000008, IC Loss: 0.000107\n",
      "Epoch 5632, Total Loss: 0.000148, PDE Loss: 0.000009, IC Loss: 0.000139\n",
      "Epoch 6144, Total Loss: 0.000017, PDE Loss: 0.000004, IC Loss: 0.000013\n",
      "Epoch 6656, Total Loss: 0.000197, PDE Loss: 0.000145, IC Loss: 0.000052\n",
      "Epoch 7168, Total Loss: 0.000995, PDE Loss: 0.000305, IC Loss: 0.000691\n",
      "Epoch 7680, Total Loss: 0.000422, PDE Loss: 0.000048, IC Loss: 0.000375\n",
      "Epoch 8192, Total Loss: 0.000326, PDE Loss: 0.000028, IC Loss: 0.000298\n",
      "Epoch 8704, Total Loss: 0.000214, PDE Loss: 0.000016, IC Loss: 0.000198\n",
      "Epoch 9216, Total Loss: 0.000162, PDE Loss: 0.000011, IC Loss: 0.000151\n",
      "Epoch 9728, Total Loss: 0.000121, PDE Loss: 0.000009, IC Loss: 0.000112\n",
      "Training completed in 2120.24 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=8, neurons=128, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.619180, PDE Loss: 0.000297, IC Loss: 0.618884\n",
      "Epoch 512, Total Loss: 0.002814, PDE Loss: 0.000001, IC Loss: 0.002813\n",
      "Epoch 1024, Total Loss: 0.000425, PDE Loss: 0.000023, IC Loss: 0.000402\n",
      "Epoch 1536, Total Loss: 0.000058, PDE Loss: 0.000005, IC Loss: 0.000053\n",
      "Epoch 2048, Total Loss: 0.000045, PDE Loss: 0.000008, IC Loss: 0.000037\n",
      "Epoch 2560, Total Loss: 0.000116, PDE Loss: 0.000065, IC Loss: 0.000050\n",
      "Epoch 3072, Total Loss: 0.000013, PDE Loss: 0.000002, IC Loss: 0.000011\n",
      "Epoch 3584, Total Loss: 0.000018, PDE Loss: 0.000005, IC Loss: 0.000013\n",
      "Epoch 4096, Total Loss: 0.000028, PDE Loss: 0.000008, IC Loss: 0.000020\n",
      "Epoch 4608, Total Loss: 0.000023, PDE Loss: 0.000002, IC Loss: 0.000022\n",
      "Training completed in 521.81 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=8, neurons=128, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.659438, PDE Loss: 0.000358, IC Loss: 0.659079\n",
      "Epoch 512, Total Loss: 0.001239, PDE Loss: 0.000394, IC Loss: 0.000845\n",
      "Epoch 1024, Total Loss: 0.000447, PDE Loss: 0.000045, IC Loss: 0.000402\n",
      "Epoch 1536, Total Loss: 0.000222, PDE Loss: 0.000018, IC Loss: 0.000204\n",
      "Epoch 2048, Total Loss: 0.000122, PDE Loss: 0.000007, IC Loss: 0.000115\n",
      "Epoch 2560, Total Loss: 0.000323, PDE Loss: 0.000105, IC Loss: 0.000218\n",
      "Epoch 3072, Total Loss: 0.000032, PDE Loss: 0.000003, IC Loss: 0.000029\n",
      "Epoch 3584, Total Loss: 0.000023, PDE Loss: 0.000004, IC Loss: 0.000019\n",
      "Epoch 4096, Total Loss: 0.000012, PDE Loss: 0.000001, IC Loss: 0.000011\n",
      "Epoch 4608, Total Loss: 0.000076, PDE Loss: 0.000011, IC Loss: 0.000065\n",
      "Epoch 5120, Total Loss: 0.000021, PDE Loss: 0.000002, IC Loss: 0.000018\n",
      "Epoch 5632, Total Loss: 0.000015, PDE Loss: 0.000004, IC Loss: 0.000011\n",
      "Epoch 6144, Total Loss: 0.000010, PDE Loss: 0.000001, IC Loss: 0.000008\n",
      "Epoch 6656, Total Loss: 0.000012, PDE Loss: 0.000001, IC Loss: 0.000011\n",
      "Epoch 7168, Total Loss: 0.000008, PDE Loss: 0.000001, IC Loss: 0.000007\n",
      "Epoch 7680, Total Loss: 0.000009, PDE Loss: 0.000001, IC Loss: 0.000008\n",
      "Epoch 8192, Total Loss: 0.000014, PDE Loss: 0.000001, IC Loss: 0.000012\n",
      "Epoch 8704, Total Loss: 0.000016, PDE Loss: 0.000001, IC Loss: 0.000015\n",
      "Epoch 9216, Total Loss: 0.000071, PDE Loss: 0.000015, IC Loss: 0.000056\n",
      "Epoch 9728, Total Loss: 0.000013, PDE Loss: 0.000006, IC Loss: 0.000008\n",
      "Training completed in 1042.03 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=8, neurons=256, epochs=5120, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.003429, PDE Loss: 0.000002, IC Loss: 0.003427\n",
      "Epoch 512, Total Loss: 0.000443, PDE Loss: 0.000113, IC Loss: 0.000330\n",
      "Epoch 1024, Total Loss: 0.000141, PDE Loss: 0.000011, IC Loss: 0.000130\n",
      "Epoch 1536, Total Loss: 0.000066, PDE Loss: 0.000006, IC Loss: 0.000060\n",
      "Epoch 2048, Total Loss: 0.000190, PDE Loss: 0.000145, IC Loss: 0.000045\n",
      "Epoch 2560, Total Loss: 0.000167, PDE Loss: 0.000142, IC Loss: 0.000025\n",
      "Epoch 3072, Total Loss: 0.000162, PDE Loss: 0.000142, IC Loss: 0.000019\n",
      "Epoch 3584, Total Loss: 0.000159, PDE Loss: 0.000143, IC Loss: 0.000016\n",
      "Epoch 4096, Total Loss: 0.000161, PDE Loss: 0.000151, IC Loss: 0.000010\n",
      "Epoch 4608, Total Loss: 0.000160, PDE Loss: 0.000149, IC Loss: 0.000011\n",
      "Training completed in 1376.05 seconds.\n",
      "\n",
      "Running experiment: activation=softplus, hidden_layers=8, neurons=256, epochs=10240, collocation_points=2048\n",
      "Piecewise initial condition: u0(x) = 2/(3pi) for x in [pi/2, 3pi/2]; 1/(3pi) otherwise\n",
      "\n",
      "Epoch 0, Total Loss: 0.069717, PDE Loss: 0.000000, IC Loss: 0.069717\n",
      "Epoch 512, Total Loss: 0.000243, PDE Loss: 0.000034, IC Loss: 0.000209\n",
      "Epoch 1024, Total Loss: 0.000087, PDE Loss: 0.000007, IC Loss: 0.000080\n",
      "Epoch 1536, Total Loss: 0.000054, PDE Loss: 0.000012, IC Loss: 0.000043\n",
      "Epoch 2048, Total Loss: 0.000041, PDE Loss: 0.000009, IC Loss: 0.000032\n",
      "Epoch 2560, Total Loss: 0.000027, PDE Loss: 0.000003, IC Loss: 0.000024\n",
      "Epoch 3072, Total Loss: 0.000025, PDE Loss: 0.000004, IC Loss: 0.000021\n",
      "Epoch 3584, Total Loss: 0.000015, PDE Loss: 0.000001, IC Loss: 0.000015\n",
      "Epoch 4096, Total Loss: 0.000165, PDE Loss: 0.000138, IC Loss: 0.000027\n",
      "Epoch 4608, Total Loss: 0.000183, PDE Loss: 0.000147, IC Loss: 0.000035\n",
      "Epoch 5120, Total Loss: 0.000156, PDE Loss: 0.000148, IC Loss: 0.000008\n",
      "Epoch 5632, Total Loss: 0.000161, PDE Loss: 0.000133, IC Loss: 0.000028\n",
      "Epoch 6144, Total Loss: 0.000224, PDE Loss: 0.000159, IC Loss: 0.000066\n",
      "Epoch 6656, Total Loss: 0.000214, PDE Loss: 0.000156, IC Loss: 0.000058\n",
      "Epoch 7168, Total Loss: 0.000193, PDE Loss: 0.000124, IC Loss: 0.000069\n",
      "Epoch 7680, Total Loss: 0.000165, PDE Loss: 0.000143, IC Loss: 0.000022\n",
      "Epoch 8192, Total Loss: 0.000156, PDE Loss: 0.000147, IC Loss: 0.000008\n",
      "Epoch 8704, Total Loss: 0.000176, PDE Loss: 0.000135, IC Loss: 0.000041\n",
      "Epoch 9216, Total Loss: 0.000193, PDE Loss: 0.000126, IC Loss: 0.000067\n",
      "Epoch 9728, Total Loss: 0.000159, PDE Loss: 0.000135, IC Loss: 0.000024\n",
      "Training completed in 2749.67 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import itertools\n",
    "\n",
    "theta_test = torch.linspace(xmin, xmax, Nx, device=device)\n",
    "t_test = torch.linspace(0, Tf, Nt, device=device)\n",
    "\n",
    "Theta, T = torch.meshgrid(theta_test, t_test, indexing='ij')  # Theta: (Nx, Nt), T: (Nx, Nt)\n",
    "\n",
    "# Input (flattened)\n",
    "x_test = torch.cat([Theta.reshape(-1, 1), T.reshape(-1, 1)], dim=1)\n",
    "\n",
    "# for initalise\n",
    "theta_init = torch.linspace(xmin, xmax, Nx, device=device).view(-1, 1)\n",
    "t_init = torch.zeros_like(theta_init)\n",
    "\n",
    "\n",
    "# Plot prediciton\n",
    "t_test_np = t_test.cpu().numpy()\n",
    "x_line = theta_test.cpu().numpy()\n",
    "Theta_np = Theta.cpu().numpy()\n",
    "T_np = T.cpu().numpy()\n",
    "\n",
    "\n",
    "def trainer__(u_0, model, optimizer, theta_colloc, t_colloc, num_epochs=10240, plot_losses=False):\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute PDE residual loss\n",
    "        f = pde_residual(model, theta_colloc, t_colloc)\n",
    "        loss_pde = torch.mean(f**2)\n",
    "        \n",
    "        # Compute initial condition loss\n",
    "        x_init = torch.cat([theta_init, t_init], dim=1)\n",
    "        u0_pred = model(x_init)\n",
    "        loss_ic = torch.mean((u0_pred - u_0)**2)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = loss_pde + loss_ic\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "        if epoch % 512 == 0:\n",
    "            print(f\"Epoch {epoch}, Total Loss: {loss.item():.6f}, PDE Loss: {loss_pde.item():.6f}, IC Loss: {loss_ic.item():.6f}\")\n",
    "    \n",
    "    if plot_losses:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(loss_history)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training Loss\")\n",
    "        plt.show()\n",
    "    \n",
    "    return loss_history\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------ #\n",
    "\n",
    "\n",
    "INITIAL = 'piecewise'\n",
    "def run_experiments(activation_funcs, hidden_layers_list, neurons_list, epochs_list, collocation_points_list):\n",
    "    save_dir = f\"{INITIAL}_experiment_results\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for (act_name, act_func), hidden_layers, neurons, epochs, cp in itertools.product(\n",
    "            activation_funcs, hidden_layers_list, neurons_list, epochs_list, collocation_points_list):\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"\\nRunning experiment: activation={act_name}, hidden_layers={hidden_layers}, neurons={neurons}, epochs={epochs}, collocation_points={cp}\")\n",
    "        \n",
    "        # Training\n",
    "        theta_colloc_exp = torch.rand(cp, 1, device=device) * xmax\n",
    "        t_colloc_exp = torch.rand(cp, 1, device=device) * Tf\n",
    "        \n",
    "        model = PINN(activation=act_func, hidden_layers=hidden_layers, neurons=neurons).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        \n",
    "        u0 = initialise(theta_init)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        loss_history = trainer__(u0, model, optimizer, theta_colloc_exp, t_colloc_exp, num_epochs=epochs, plot_losses=False)\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "        print(f\"Training completed in {training_time:.2f} seconds.\")\n",
    "        \n",
    "\n",
    "        # Prediciton\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            u_n = model(x_test).reshape(Nx, Nt).cpu().numpy()\n",
    "        \n",
    "\n",
    "        # Plotting\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))        \n",
    "        # --- Contour plot ---\n",
    "        cp_plot = axes[0].contourf(Theta_np, T_np, u_n, levels=50, cmap='viridis')\n",
    "        fig.colorbar(cp_plot, ax=axes[0], label='p(x,t)')\n",
    "        axes[0].set_xlabel(r'$\\theta$')\n",
    "        axes[0].set_ylabel('t')\n",
    "        axes[0].set_title(f'Contour: {act_name}, HL:{hidden_layers}, N:{neurons}, epochs:{epochs}, cp:{cp}')\n",
    "        \n",
    "        # --- Line plot ---\n",
    "        time_snapshots = [0.0, Tf/4, Tf/2, 3*Tf/4, Tf]\n",
    "        for ts in time_snapshots:\n",
    "            idx = np.argmin(np.abs(t_test_np - ts))\n",
    "            axes[1].plot(x_line, u_n[:, idx], label=f\"t={t_test_np[idx]:.2f}\")\n",
    "        axes[1].set_xlabel(r'$\\theta$')\n",
    "        axes[1].set_ylabel(r'$\\rho(\\theta)$')\n",
    "        axes[1].set_title(f'Line Plot: {act_name}, HL:{hidden_layers}, N:{neurons}, epochs:{epochs}, cp:{cp}, time:{training_time:.2f}s')\n",
    "        axes[1].legend(fontsize=8)\n",
    "        \n",
    "        # Save plots\n",
    "        exp_filename = f\"solution_{act_name}_HL{hidden_layers}_N{neurons}_epochs{epochs}_cp{cp}.png\"\n",
    "        fig.savefig(os.path.join(save_dir, exp_filename))\n",
    "        plt.close(fig)    \n",
    "\n",
    "\n",
    "# -------------------------------------- #\n",
    "\n",
    "# Parameter grids.\n",
    "activation_funcs = [\n",
    "    (\"relu\", torch.relu),\n",
    "    (\"leaky_relu\", nn.functional.leaky_relu),\n",
    "    (\"sin\", lambda x: torch.sin(x)),\n",
    "    (\"softplus\", nn.functional.softplus)\n",
    "]\n",
    "hidden_layers_list = [6, 8]\n",
    "neurons_list = [128, 256]\n",
    "epochs_list = [5120, 10240]\n",
    "collocation_points_list = [2048]\n",
    "\n",
    "run_experiments(activation_funcs, hidden_layers_list, neurons_list, epochs_list, collocation_points_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
